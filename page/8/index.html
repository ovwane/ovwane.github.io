<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>幻舞梦境</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="幻舞梦境">
<meta property="og:url" content="http://ovwane.me/page/8/index.html">
<meta property="og:site_name" content="幻舞梦境">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="幻舞梦境">
  
    <link rel="alternative" href="/atom.xml" title="幻舞梦境" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/favicon.png">
  
  
      <link rel="stylesheet" href="//cdn.bootcss.com/animate.css/3.5.0/animate.min.css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/font-awesome/css/font-awesome.min.css">
  <link rel="apple-touch-icon" href="/apple-touch-icon.png">
  
     
      <meta name="baidu-site-verification" content="3rC08I0Cp6" />
    
     
      <meta name="google-site-verification" content="rn8f25RzFQoiByt8ezg9E17KZIElbIlwJ4y-EKSlWbA" />
    
  
  
      <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  
  <!-- 加载特效 -->
    <script src="/js/pace.js"></script>
    <link href="/css/pace/pace-theme-flash.css" rel="stylesheet" />
  <script>
      var yiliaConfig = {
          rootUrl: '/',
          fancybox: true,
          animate: true,
          isHome: true,
          isPost: false,
          isArchive: false,
          isTag: false,
          isCategory: false,
          open_in_new: false
      }
  </script>
</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            
            <img lazy-src="/img/head.jpg" class="js-avatar">
            
        </a>

        <hgroup>
          <h1 class="header-author"><a href="/" title="Hi Mate">幻舞梦境</a></h1>
        </hgroup>

        
        
        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">首页</a></li>
                        
                            <li><a href="/WebGuide">导航</a></li>
                        
                            <li><a href="/ovwane_love">Love</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fl github" target="_blank" href="https://github.com/ovwane" title="github">github</a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <a href="/tags/Ansible/" style="font-size: 11.43px;">Ansible</a> <a href="/tags/Bind/" style="font-size: 10px;">Bind</a> <a href="/tags/Cacti/" style="font-size: 10px;">Cacti</a> <a href="/tags/CentOS/" style="font-size: 20px;">CentOS</a> <a href="/tags/Cobbler/" style="font-size: 10px;">Cobbler</a> <a href="/tags/DHCP/" style="font-size: 10px;">DHCP</a> <a href="/tags/Django/" style="font-size: 12.86px;">Django</a> <a href="/tags/Docker/" style="font-size: 10px;">Docker</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Gitlab/" style="font-size: 11.43px;">Gitlab</a> <a href="/tags/HAProxy/" style="font-size: 10px;">HAProxy</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/Homebrew/" style="font-size: 10px;">Homebrew</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Jenkins/" style="font-size: 11.43px;">Jenkins</a> <a href="/tags/KVM/" style="font-size: 10px;">KVM</a> <a href="/tags/Kubernetes/" style="font-size: 10px;">Kubernetes</a> <a href="/tags/Linux/" style="font-size: 15.71px;">Linux</a> <a href="/tags/MariaDB/" style="font-size: 10px;">MariaDB</a> <a href="/tags/MySQL/" style="font-size: 10px;">MySQL</a> <a href="/tags/NFS/" style="font-size: 10px;">NFS</a> <a href="/tags/Nagios/" style="font-size: 10px;">Nagios</a> <a href="/tags/Nginx/" style="font-size: 17.14px;">Nginx</a> <a href="/tags/Oh-My-Zsh/" style="font-size: 10px;">Oh My Zsh</a> <a href="/tags/OpenLDAP/" style="font-size: 10px;">OpenLDAP</a> <a href="/tags/OpenStack/" style="font-size: 10px;">OpenStack</a> <a href="/tags/PHP/" style="font-size: 11.43px;">PHP</a> <a href="/tags/PXE/" style="font-size: 10px;">PXE</a> <a href="/tags/Python/" style="font-size: 15.71px;">Python</a> <a href="/tags/RHEL7/" style="font-size: 10px;">RHEL7</a> <a href="/tags/Redis/" style="font-size: 10px;">Redis</a> <a href="/tags/Requests/" style="font-size: 10px;">Requests</a> <a href="/tags/Resin/" style="font-size: 10px;">Resin</a> <a href="/tags/Samba/" style="font-size: 10px;">Samba</a> <a href="/tags/Scrapy/" style="font-size: 11.43px;">Scrapy</a> <a href="/tags/Scrapyd/" style="font-size: 10px;">Scrapyd</a> <a href="/tags/SpiderKeeper/" style="font-size: 10px;">SpiderKeeper</a> <a href="/tags/Squid/" style="font-size: 10px;">Squid</a> <a href="/tags/Subversion/" style="font-size: 10px;">Subversion</a> <a href="/tags/Supervisor/" style="font-size: 10px;">Supervisor</a> <a href="/tags/Tomcat/" style="font-size: 12.86px;">Tomcat</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/Web/" style="font-size: 10px;">Web</a> <a href="/tags/Xadmin/" style="font-size: 10px;">Xadmin</a> <a href="/tags/Zabbix/" style="font-size: 10px;">Zabbix</a> <a href="/tags/cacti/" style="font-size: 10px;">cacti</a> <a href="/tags/dovecot/" style="font-size: 10px;">dovecot</a> <a href="/tags/httpd/" style="font-size: 11.43px;">httpd</a> <a href="/tags/iSCSI/" style="font-size: 10px;">iSCSI</a> <a href="/tags/k8s/" style="font-size: 10px;">k8s</a> <a href="/tags/kickstart/" style="font-size: 10px;">kickstart</a> <a href="/tags/macOS/" style="font-size: 14.29px;">macOS</a> <a href="/tags/nagios/" style="font-size: 10px;">nagios</a> <a href="/tags/postfix/" style="font-size: 10px;">postfix</a> <a href="/tags/rsync/" style="font-size: 10px;">rsync</a> <a href="/tags/sshd/" style="font-size: 10px;">sshd</a> <a href="/tags/uWSGI/" style="font-size: 10px;">uWSGI</a> <a href="/tags/vsftpd/" style="font-size: 10px;">vsftpd</a> <a href="/tags/zabbix/" style="font-size: 10px;">zabbix</a> <a href="/tags/历史/" style="font-size: 18.57px;">历史</a> <a href="/tags/情感/" style="font-size: 10px;">情感</a> <a href="/tags/监控/" style="font-size: 12.86px;">监控</a> <a href="/tags/自己/" style="font-size: 10px;">自己</a> <a href="/tags/运维/" style="font-size: 10px;">运维</a>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a target="_blank" class="main-nav-link switch-friends-link" href="http://luuman.github.io/">name</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">纯海迷、爱运动、爱交友、爱旅行、喜欢接触新鲜事物、迎接新的挑战，更爱游离于错综复杂的编码与逻辑中</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="Me">幻舞梦境</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                
                    <img lazy-src="/img/head.jpg" class="js-avatar">
                
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="Me">幻舞梦境</a></h1>
            </hgroup>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">首页</a></li>
                
                    <li><a href="/WebGuide">导航</a></li>
                
                    <li><a href="/ovwane_love">Love</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                <div class="social">
                    
                        <a class="github" target="_blank" href="https://github.com/ovwane" title="github">github</a>
                    
                </div>
            </nav>
        </header>                
    </div>
</nav>
      <div class="body-wrap">
  
    <article id="post-Python学习-老男孩系的学生们的博客" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/20/Python学习-老男孩系的学生们的博客/" class="article-date">
      <time datetime="2018-04-20T08:14:15.867Z" itemprop="datePublished">2018-04-20</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p><a href="http://www.cnblogs.com/yangjian1/p/5917621.html" target="_blank" rel="noopener">python基础之初始python</a></p>
<p><a href="http://www.cnblogs.com/allen-zhang/p/6114864.html" target="_blank" rel="noopener">Python之路,Day1 - Python基础1</a></p>

      
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-PowerShell使用笔记" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/20/PowerShell使用笔记/" class="article-date">
      <time datetime="2018-04-20T08:14:15.867Z" itemprop="datePublished">2018-04-20</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>#查看PowerShell版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#查看 PSVersion版本，就是PowerShell的版本。</span><br><span class="line">$PSVersionTable</span><br></pre></td></tr></table></figure></p>

      
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-PyCharm新建项目并使用Github管理代码" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/20/PyCharm新建项目并使用Github管理代码/" class="article-date">
      <time datetime="2018-04-20T08:14:15.867Z" itemprop="datePublished">2018-04-20</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <ul>
<li>在Github上申请token</li>
</ul>
<p><a href="https://github.com/settings/tokens" target="_blank" rel="noopener">Github Personal access tokens</a></p>
<ul>
<li><p>pycharm 设置github token<br>Preferences-&gt;Version Control-&gt;GIthub</p>
</li>
<li><p>pycharm上新建project，名字 python_crawler。</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /Users/jinlong/PycharmProjects/python_crawler</span><br><span class="line"></span><br><span class="line">pipenv --python=/Users/jinlong/.pyenv/versions/3.6.3/bin/python</span><br><span class="line"></span><br><span class="line">sed -i &quot;&quot; &quot;s/python.org/douban.com/g&quot; Pipfile</span><br></pre></td></tr></table></figure>
<ul>
<li><p>设置pycharm的python解释器<br>Preferences-&gt;Project:python_crawler-&gt;Project Interpreter</p>
</li>
<li><p>设置pycharm下的github</p>
</li>
</ul>
<p>VCS-&gt;Import into Version Control-&gt;Share Project On Github</p>
<h2 id="PyCharm-配置"><a href="#PyCharm-配置" class="headerlink" title="PyCharm 配置"></a>PyCharm 配置</h2><ul>
<li><p>Tab键设置成4个空格<br>Preferences-&gt;Editor-&gt;Code Style-&gt;Python-&gt;Use tab character 取消勾选</p>
</li>
<li><p>调整字母长度分割线<br>Preferences-&gt;Editor-&gt;Code Style-&gt;Right margin（columns) 设置为80</p>
</li>
<li><p>文件模板<br>Preferences-&gt;Editor-&gt;File and Code Templates</p>
</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line">__author__ = <span class="string">'ovwane'</span></span><br><span class="line">__date__ = <span class="string">'$DATE $HOUR:$MINUTE'</span></span><br></pre></td></tr></table></figure>
<p><a href="https://www.cnblogs.com/hester/p/5466579.html" target="_blank" rel="noopener">Pycharm安装、设置、优化</a></p>
<p><a href="http://zh-google-styleguide.readthedocs.io/en/latest/google-python-styleguide/contents/" target="_blank" rel="noopener">Google 开源项目风格指南-Python 风格指南 - 内容目录</a></p>
<h2 id="PyCharm插件"><a href="#PyCharm插件" class="headerlink" title="PyCharm插件"></a>PyCharm插件</h2><p>Mongo Plugin</p>
<p>Markdown Navigator<br>Markdown support</p>
<h2 id="Python工具"><a href="#Python工具" class="headerlink" title="Python工具"></a>Python工具</h2><p>nmap<br><code>pip install python-nmap</code></p>

      
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Pyhont学习网站" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/20/Pyhont学习网站/" class="article-date">
      <time datetime="2018-04-20T08:14:15.867Z" itemprop="datePublished">2018-04-20</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p><a href="http://bbs.fishc.com/forum-173-1.html" target="_blank" rel="noopener">鱼C工作室-Python教学</a></p>
<p><a href="https://cuiqingcai.com/" target="_blank" rel="noopener">静觅丨崔庆才的个人博客</a></p>
<p><a href="https://www.lijinlong.cc/fuwuqi/hjpz/1962.html" target="_blank" rel="noopener">Django、Python3、Nginx、uWSGI环境部署教程</a></p>

      
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Python 3.x模块" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/20/Python 3.x模块/" class="article-date">
      <time datetime="2018-04-20T08:14:15.867Z" itemprop="datePublished">2018-04-20</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Python 模块</p>
<p><a href="https://docs.python.org/3/py-modindex.html" target="_blank" rel="noopener">Python 3.x Module Index</a></p>
<p>urllib</p>
<p>hashlib<br>cookielib<br>re</p>
<p>getpass<br>json<br>pickle<br>pickletools<br>functools<br>glob</p>
<h3 id="configparser"><a href="#configparser" class="headerlink" title="configparser"></a>configparser</h3><p><a href="http://blog.csdn.net/miner_k/article/details/77857292" target="_blank" rel="noopener"> python的ConfigParser模块</a></p>
<h3 id="logging"><a href="#logging" class="headerlink" title="logging"></a>logging</h3><p><a href="http://zmister.com/archives/213.html" target="_blank" rel="noopener">优雅地记录Python程序日志1：logging模块简介</a></p>
<p><a href="http://zmister.com/archives/217.html" target="_blank" rel="noopener">优雅地记录Python程序日志2：模块组件化日志记录器</a></p>
<p>os<br>sys<br>queue<br>random<br>socket<br>socketserver<br>threading<br>multiprocessing</p>
<p>第三方模块</p>
<p>Requests<br>Scrapy<br>Pillow<br>BeautifulSoup<br>lxml</p>
<h3 id="selenium"><a href="#selenium" class="headerlink" title="selenium"></a>selenium</h3><p>胡志恒<br><a href="http://www.cnblogs.com/fnng/" target="_blank" rel="noopener">虫师</a></p>
<p><a href="http://www.cnblogs.com/fnng/p/7797839.html" target="_blank" rel="noopener">Chrome headless 模式</a></p>
<p><a href="http://blog.csdn.net/zhaoxz1985/article/details/72780085" target="_blank" rel="noopener">Web接口开发与自动化测试基于Python语言–第1章</a></p>
<p><a href="http://blog.csdn.net/zhaoxz1985/article/details/72862466" target="_blank" rel="noopener">Web接口开发与自动化测试基于Python语言–第2章</a></p>

      
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Python+Web开发实战-董伟明-目录" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/20/Python+Web开发实战-董伟明-目录/" class="article-date">
      <time datetime="2018-04-20T08:14:15.867Z" itemprop="datePublished">2018-04-20</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>页码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">486 第15章 Web 开发项目实践</span><br></pre></td></tr></table></figure>
      
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Python+Web开发实战-董伟明-笔记" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/20/Python+Web开发实战-董伟明-笔记/" class="article-date">
      <time datetime="2018-04-20T08:14:15.867Z" itemprop="datePublished">2018-04-20</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>页码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">486 第15章 Web 开发项目实践</span><br></pre></td></tr></table></figure>
      
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Python分布式爬虫打造搜索引擎笔记" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/20/Python分布式爬虫打造搜索引擎笔记/" class="article-date">
      <time datetime="2018-04-20T08:14:15.867Z" itemprop="datePublished">2018-04-20</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>2018.03.19 20:01</p>
<p>作者博客<br><a href="http://projectsedu.com" target="_blank" rel="noopener">http://projectsedu.com</a><br>github.com/liyaopinner</p>
<p>IDE pycharm<br>数据库 mysql redis </p>
<p>Python 2.7.13<br>Python 3.5.3</p>
<p>第三章 基础知识</p>
<p>技术选型:<br>scrapy<br>requests<br>scrapy selector</p>
<p>网页分类：<br>静态网页<br>动态网页<br>微博service(rest api)</p>
<p>爬虫能做什么<br>1、搜索引擎-百度<br>2、推荐引擎-今日头条<br>3、机器学习的数据样本<br>4、数据分析、舆情分析</p>
<p>正则表达式<br>非贪婪模式 ？<br>至少出现一次 +<br>限定前面的字符出现的次数 {}<br>满足任意一个 []<br>\s 代表空格<br>\S 只要不是空格就行一个字符<br>\w 代表a-zA-Z0-9_ 代表字符<br>\W 不代表a-Z0-9_<br>\d 代表数字</p>
<p>深度优先和广度优先算法</p>
<p>深度优先 递归实现<br>广度优先 队列实现</p>
<p>数据结构里很重要的算法</p>
<p>深度优先过程</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">depth_tree</span><span class="params">(tree_node)</span>:</span></span><br><span class="line">	<span class="keyword">if</span> tree_node <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">		print(tree_node._data)</span><br><span class="line">		<span class="keyword">if</span> tree_node._left <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">			<span class="keyword">return</span> depth_tree(tree_node._left):</span><br><span class="line">		<span class="keyword">if</span> tree_node._right <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">			<span class="keyword">return</span> depth_tree(tree_node._right)</span><br></pre></td></tr></table></figure>
<p>广度优先过程</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">level_queue</span><span class="params">(root)</span>:</span></span><br><span class="line">	<span class="keyword">if</span> root <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	my_queue = []</span><br><span class="line">	node = root</span><br><span class="line">	my_queue.append(node)</span><br><span class="line">	<span class="keyword">while</span> my_queue:</span><br><span class="line">		node = my_queue.pop(<span class="number">0</span>)</span><br><span class="line">		print(node.elem)</span><br><span class="line">		<span class="keyword">if</span> node.lchild <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">			my_queue.append(node.lchild)</span><br><span class="line">		<span class="keyword">if</span> node.rchild <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">			my_queue.append(node.rchild)</span><br></pre></td></tr></table></figure>
<p>url去重策略</p>
<p>1、将访问过的url保存到数据库<br>2、将访问过的url保存到set中<br>3、url经过md5等方法哈希后保存到set中 scrapy使用的是这个方法<br>4、用bitmap方法，<br>5、bloomfilter方法对bitmap进行改进，多重hash函数降低冲突</p>
<p>unicode和utf8编码<br>内存中的数据都是unicode编码<br>python3中文件中存的数据是utf-8编码<br>所有编码解码 decode为unicode<br>uncode编码 encode为utf-8</p>
<p>获取系统默认的编码<br>sys.getdefaultencoding()</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-*- coding: utf<span class="number">-8</span> -*-</span><br></pre></td></tr></table></figure>
<p>scrapy爬取jobbole</p>
<p>pycharm 中调试scrapy</p>
<p>main.py main.py放在和scrapy.cfg同一目录</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.cmdline <span class="keyword">import</span> execute</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找main.py的执行位置</span></span><br><span class="line">sys.path.append(os.path.dirname(os.path.abspath(__file__)))</span><br><span class="line"><span class="comment"># 执行爬虫</span></span><br><span class="line">execute([<span class="string">'scrapy'</span>, <span class="string">'crawl'</span>, <span class="string">'jobbole'</span>])</span><br></pre></td></tr></table></figure>
<p>xpath语法<br>div[@class=”goods-add fn-clear J-DAddToBag”]<br>//span[contains(@class, ‘J-DAddToBag’)]</p>
<p>/article/div[1] 选取属于article子元素的第一个div元素<br>/article/div[last()] 选取属于article子元素的最后一个div元素<br>/article/div[last()-1] 选取属于article子元素的倒数第二个div元素<br>//div/<em> 选取属于div元素的所有子节点<br>//div[@</em>] 选取所有带属性的title元素<br>/div/a|//div/p 选取所有div元素的a和p元素<br>//span|//ul 选取文档中的span和ul元素<br>article/div/p| //span 选取所有属于span元素的div元素的p元素以及文档中的所有的span元素</p>
<p>css selector语法<br>li a 选取所有li下的所有a元素<br>ul + p 选择ul后面的第一个p元素<br>div#container &gt; ul 选取id为container的div的第一个ul子元素<br>ul ~ p 选取与ul相邻的所有p元素<br>a[title] 选取所有有title属性的a元素<br>a[href=”<a href="http://jobbole.com&quot;]" target="_blank" rel="noopener">http://jobbole.com&quot;]</a> 选取所有href属性为jobbole.com值的a元素<br>a[href*=”jobbole”] 选取所有href属性包含jobbole.com的a元素<br>a[href^=”http”] 选取所有href属性以http开头的a元素<br>a[href$=”.jpg”] 选取所有href属性以.jpg结尾的a元素<br>input[type=radio]:checked 选择选中的radio的元素<br>div:not(#container) 选取所有id非container属性的div元素<br>li:nth-child(3) 选取第三个li元素<br>tr:nth-child(2n) 第偶数个tr</p>
<p>伪类选择器<br>h1::text<br>div h1::attr(href) </p>
<p>列表生成式<br>tag_list = [‘职场’, ‘1 评论’, ‘fuck两点水’]</p>
<p>[ element for element in tag_list if not element.strip().endswith(“评论”)]</p>
<p>正则表达式<br>fav_nums = “sssd123werwe”<br>match_re = re.match(“.<em>(\d+).</em>“, fav_nums)<br>if match_re:<br>    fav_nums = int(match_re.group(1))<br>else:<br>    fav_nums = 0</p>
<p>字符串方法<br>tags = “,”.join(‘12345’)        结果 ‘1,2,3,4,5’</p>
<p>response.xpath().extract() 需要异常处理<br>response.xpath().extract_first()</p>
<p>from scrapy.http import Request<br>Request(url=,callback=)</p>
<p>from urllib import parse<br>parse.urljoin()</p>
<p>meta<br>scrapy.Request(url=,meta={‘goodsid’:goodsid},callback=)<br>goodsid = response.meta.get(“goodsid”, “”)</p>
<p>scrapy 图片管道 imagespipeline <code>pip install pillow</code><br>settings.py<br>scrapy.pipelines.images.ImagesPipeLine<br>IMAGES_URLS_FIELD = “items.py 中的item,url需要是list类型”<br>project_dir = os.path.abspath(os.path.dirname(<strong>file</strong>))<br>IMAGES_STORE = os.path.join(project_dir, ‘images’)</p>
<p><a href="https://coding.imooc.com/lesson/92.html#mid=2878" target="_blank" rel="noopener">DIY pipeline</a><br>图片管道自定义<br>4-12 items设计-3 视频的02:12</p>
<p>ImagesPipeline<br>item_completed()</p>
<p>utils/<br><strong>init</strong>.py<br>common.py</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_md5</span><span class="params">(url)</span>:</span></span><br><span class="line">	<span class="keyword">if</span> isinstance(url, str):</span><br><span class="line">		url = url.encode(<span class="string">"utf-8"</span>)</span><br><span class="line">	m = hashlib.md5()</span><br><span class="line">	m.update(url)</span><br><span class="line">	<span class="keyword">return</span> m.hexdigest()</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">	print(get_md5(<span class="string">"http://www.baidu.com"</span>)</span><br></pre></td></tr></table></figure>
<p>数据保存<br>pipeline</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">		self.file = codecs.open(<span class="string">'article.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">"utf-8"</span>)</span><br><span class="line">		</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">	lines = json.dumps(dict(item), ensure_ascii=<span class="keyword">False</span>) + <span class="string">"\n"</span></span><br><span class="line">	self.write(lines)</span><br><span class="line">	<span class="keyword">return</span> item</span><br><span class="line">	</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">            self.file.close()</span><br></pre></td></tr></table></figure>
<p>JsonItemExporter</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> JsonItemExporter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonExporterPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">		self.file = open(<span class="string">'articleexporter.json'</span>, <span class="string">'wb'</span>)</span><br><span class="line">		self.exporter = JsonItemExporter(self.file, encoding=<span class="string">"utf-8"</span>, ensure_ascii=<span class="keyword">False</span>)</span><br><span class="line">		self.exporter.start_exporting()</span><br><span class="line">	</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self. item, spider)</span>:</span></span><br><span class="line">		self.exporter.export_item(item)</span><br><span class="line">		<span class="keyword">return</span> item</span><br><span class="line">		</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">		self.exporter.finish_exporting()</span><br><span class="line">		self.file.close()</span><br></pre></td></tr></table></figure>
<p>数据库</p>
<p>新建数据库<br>新建数据表</p>
<p><img src="./jobbole_article_mysql.png" alt="jobbole_article_mysql"></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create_date = <span class="string">'时间字符串'</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">	datetime.datetime.strptime(create_date, <span class="string">"%Y/%m/%d"</span>).date()</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">	create_date = datetime.datetime.now().date()</span><br></pre></td></tr></table></figure>
<p>pip install mysqlclient pymysql</p>
<p>同步插入mysql</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">		self.conn = pymysql.connect(<span class="string">'host'</span>, <span class="string">'user'</span>, <span class="string">'password'</span>, <span class="string">'port'</span>, <span class="string">'dbname'</span>, charset=<span class="string">"utf-8"</span>, use_unicode=Ture)</span><br><span class="line">		self.cursor = self.conn.cursor()</span><br><span class="line">		</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">		insert_sql = <span class="string">"""</span></span><br><span class="line"><span class="string">			insert into jobbole_article(title, url)</span></span><br><span class="line"><span class="string">			VALUES (%s,%s)"""</span></span><br><span class="line">		self.cursor.execute(insert_sql, (item[<span class="string">"title"</span>], item[<span class="string">"url"</span>])</span><br><span class="line">		self.conn.commit()</span><br></pre></td></tr></table></figure>
<p>异步插入mysql</p>
<p>连接池</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> twisted.enterprise <span class="keyword">import</span> adbapi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlTwistedPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">		self.dbpool = dbpool</span><br><span class="line">	</span><br><span class="line"><span class="meta">	@classmethod</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span></span><br><span class="line">		dbparms = dict(</span><br><span class="line">			host = settings[<span class="string">"MYSQL_HOST"</span>],</span><br><span class="line">			db = settings[<span class="string">"MYSQL_DBNAME"</span>],</span><br><span class="line">			user = settings[<span class="string">"MYSQL_USER"</span>],</span><br><span class="line">			password = settings[<span class="string">"MYSQL_PASSWORD"</span>],</span><br><span class="line">			charset = <span class="string">'utf-8'</span>,</span><br><span class="line">			cursorclass = pymysql.cursors.DictCursor,</span><br><span class="line">			use_unicode = <span class="keyword">True</span>,</span><br><span class="line">		)</span><br><span class="line">		dbpool = adbapi.ConnectionPool(<span class="string">"pymysql"</span>, **dbparms)</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">return</span> cls(dbpool)</span><br><span class="line">	</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">		query = self.dbpool.runInteraction(do_insert, item)</span><br><span class="line">		query.addErrback(self.handle_error)</span><br><span class="line">		</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">handle_error</span><span class="params">(self, failure)</span>:</span></span><br><span class="line">		print(failure)</span><br><span class="line">		</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">do_insert</span><span class="params">(self, cursor, item)</span>:</span></span><br><span class="line">		insert_sql = <span class="string">"""</span></span><br><span class="line"><span class="string">			insert into jobbole_article(title, url)</span></span><br><span class="line"><span class="string">			VALUES (%s,%s)"""</span></span><br><span class="line">		cursor.execute(insert_sql, (item[<span class="string">"title"</span>], item[<span class="string">"url"</span>])</span><br></pre></td></tr></table></figure>
<p>scrapy-djangoitem</p>
<p>pip install scrapy-djangoitem</p>
<p>scrapy item loader 机制</p>
<p>itemloader会将所有提取的值变为list类型</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">item_loader = ItemLoader(item=JobboleItem(),response=response)</span><br><span class="line"></span><br><span class="line">itemloader.add_xpath()</span><br><span class="line">itemloader.add_css()</span><br><span class="line">itemloader.value()</span><br><span class="line"></span><br><span class="line">item = itemloader.load_item()</span><br><span class="line"></span><br><span class="line"><span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>
<p>items.py</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> MapCompose</span><br><span class="line"></span><br><span class="line"><span class="comment">#取第一个值</span></span><br><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> TakeFirst，Join</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_jobbole</span><span class="params">(value)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> value + <span class="string">"-jobbole"</span></span><br><span class="line">	</span><br><span class="line"></span><br><span class="line">title = scrapy.Field(</span><br><span class="line">	input_processor = MapCompose(add_jobbole)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">name = scrapy.Field(</span><br><span class="line">	input_processor = MapCompose(<span class="keyword">lambda</span> x:x+<span class="string">"-jinlong"</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">urls = scrapy.Field(</span><br><span class="line">	input_processor = MapCompose(<span class="keyword">lambda</span> x:x+<span class="string">"-kjj"</span>, add_jobbole)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">create_date = scrapy.Field(</span><br><span class="line">	input_processor = MapCompose(add_jobbole)</span><br><span class="line">	output_processor = TakeFirst()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>自定义itemloader</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleItemLoader</span><span class="params">(ItemLoader)</span>:</span></span><br><span class="line">	default_output_processor = TakeFirst()</span><br></pre></td></tr></table></figure>
<p>session和cookie自动登录机制<br>session服务器端<br>cookie客户端</p>
<p>settings.py<br>AUTOTHROTTLE_ENABLED = True</p>
<p>spider.py</p>
<p>custom_settings = {<br>    “COOKIES_ENABLED”: True<br>}</p>
<p>selenuim driver</p>
<p>ChromeDriver - WebDriver for Chrome</p>
<p><a href="https://sites.google.com/a/chromium.org/chromedriver/downloads" target="_blank" rel="noopener">https://sites.google.com/a/chromium.org/chromedriver/downloads</a></p>
<p>brew install chromedriver</p>
<p><a href="https://segmentfault.com/a/1190000013067705" target="_blank" rel="noopener">https://segmentfault.com/a/1190000013067705</a></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.options <span class="keyword">import</span> Options</span><br><span class="line">chrome_options = Options()</span><br><span class="line">chrome_options.add_argument(<span class="string">'window-size=1920x3000'</span>) <span class="comment">#指定浏览器分辨率</span></span><br><span class="line">chrome_options.add_argument(<span class="string">'--disable-gpu'</span>) <span class="comment">#谷歌文档提到需要加上这个属性来规避bug</span></span><br><span class="line">chrome_options.add_argument(<span class="string">'--hide-scrollbars'</span>) <span class="comment">#隐藏滚动条, 应对一些特殊页面</span></span><br><span class="line">chrome_options.add_argument(<span class="string">'blink-settings=imagesEnabled=false'</span>) <span class="comment">#不加载图片, 提升速度</span></span><br><span class="line">chrome_options.add_argument(<span class="string">'--headless'</span>) <span class="comment">#浏览器不提供可视化页面. linux下如果系统不支持可视化不加这条会启动失败</span></span><br><span class="line">chrome_options.binary_location = <span class="string">r'/Applications/Google Chrome Canary.app/Contents/MacOS/Google Chrome Canary'</span> <span class="comment">#手动指定使用的浏览器位置</span></span><br></pre></td></tr></table></figure>
      
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Python分布式爬虫打造搜索引擎 学习笔记" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/20/Python分布式爬虫打造搜索引擎 学习笔记/" class="article-date">
      <time datetime="2018-04-20T08:14:15.867Z" itemprop="datePublished">2018-04-20</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="Python分布式爬虫打造搜索引擎"><a href="#Python分布式爬虫打造搜索引擎" class="headerlink" title="Python分布式爬虫打造搜索引擎"></a><a href="http://coding.imooc.com/class/92.html" target="_blank" rel="noopener">Python分布式爬虫打造搜索引擎</a></h1><h2 id="第1章-课程介绍"><a href="#第1章-课程介绍" class="headerlink" title="第1章 课程介绍"></a>第1章 课程介绍</h2><h4 id="介绍课程目标、通过课程能学习到的内容、和系统开发前需要具备的知识"><a href="#介绍课程目标、通过课程能学习到的内容、和系统开发前需要具备的知识" class="headerlink" title="介绍课程目标、通过课程能学习到的内容、和系统开发前需要具备的知识"></a>介绍课程目标、通过课程能学习到的内容、和系统开发前需要具备的知识</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Scrapy </span><br><span class="line">elasticsearch</span><br><span class="line">django</span><br></pre></td></tr></table></figure>
<h2 id="第2章-windows下搭建开发环境"><a href="#第2章-windows下搭建开发环境" class="headerlink" title="第2章 windows下搭建开发环境"></a>第2章 windows下搭建开发环境</h2><h4 id="介绍项目开发需要安装的开发软件、-python虚拟virtualenv和-virtualenvwrapper的安装和使用、-最后介绍pycharm和navicat的简单使用"><a href="#介绍项目开发需要安装的开发软件、-python虚拟virtualenv和-virtualenvwrapper的安装和使用、-最后介绍pycharm和navicat的简单使用" class="headerlink" title="介绍项目开发需要安装的开发软件、 python虚拟virtualenv和 virtualenvwrapper的安装和使用、 最后介绍pycharm和navicat的简单使用"></a>介绍项目开发需要安装的开发软件、 python虚拟virtualenv和 virtualenvwrapper的安装和使用、 最后介绍pycharm和navicat的简单使用</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">PyCharm安装和配置</span><br><span class="line">MySQL和Navicat的安装使用</span><br><span class="line">CentOS安装Python2和Python3</span><br><span class="line">Python虚拟环境的安装配置virtualenv、virtualenvwrapper</span><br></pre></td></tr></table></figure>
<h2 id="第3章-爬虫基础知识回顾"><a href="#第3章-爬虫基础知识回顾" class="headerlink" title="第3章 爬虫基础知识回顾"></a>第3章 爬虫基础知识回顾</h2><h4 id="介绍爬虫开发中需要用到的基础知识包括爬虫能做什么，正则表达式，深度优先和广度优先的算法及实现、爬虫url去重的策略、彻底弄清楚unicode和utf8编码的区别和应用。"><a href="#介绍爬虫开发中需要用到的基础知识包括爬虫能做什么，正则表达式，深度优先和广度优先的算法及实现、爬虫url去重的策略、彻底弄清楚unicode和utf8编码的区别和应用。" class="headerlink" title="介绍爬虫开发中需要用到的基础知识包括爬虫能做什么，正则表达式，深度优先和广度优先的算法及实现、爬虫url去重的策略、彻底弄清楚unicode和utf8编码的区别和应用。"></a>介绍爬虫开发中需要用到的基础知识包括爬虫能做什么，正则表达式，深度优先和广度优先的算法及实现、爬虫url去重的策略、彻底弄清楚unicode和utf8编码的区别和应用。</h4><h5 id="技术选型"><a href="#技术选型" class="headerlink" title="技术选型"></a>技术选型</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scrapy vs requests+beautifulsoup</span><br><span class="line">1.requests和beautifulsoup都是库，scrapy是框架</span><br><span class="line">2.scrapy框架中可以加入requests和beautifulsoup</span><br><span class="line">3.scarpy基于twisted，性能是最大的优势</span><br><span class="line">4.scarpy方便扩展，提供了很多内置的功能</span><br><span class="line">5.scrapy内置的css和xpath selector非常方便，beautifulsoup最大的缺点就是慢</span><br></pre></td></tr></table></figure>
<h5 id="网页分类"><a href="#网页分类" class="headerlink" title="网页分类"></a>网页分类</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">常见类型的服务</span><br><span class="line">1.静态网页</span><br><span class="line">2.动态网页</span><br><span class="line">3.webservice(restapi)</span><br></pre></td></tr></table></figure>
<h5 id="爬虫能做什么"><a href="#爬虫能做什么" class="headerlink" title="爬虫能做什么"></a>爬虫能做什么</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">爬虫作用</span><br><span class="line">1.搜索引擎---百度、google、垂直领域搜索引擎</span><br><span class="line">2.推荐引擎---今日头条</span><br><span class="line">3.机器学习的数据样本</span><br><span class="line">4.数据分析（如金融数据分析）、舆情分析等</span><br></pre></td></tr></table></figure>
<h4 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">目录</span><br><span class="line">1.特殊字符</span><br><span class="line">1)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># 待匹配字符串</span></span><br><span class="line">line = <span class="string">'ssfder'</span></span><br><span class="line"><span class="comment"># 匹配规则</span></span><br><span class="line">regex_str = <span class="string">"^b.*"</span></span><br><span class="line"><span class="comment"># 匹配字符</span></span><br><span class="line"><span class="keyword">if</span> re.match(regex_str, line):</span><br><span class="line">	print(<span class="string">"yes"</span>)</span><br></pre></td></tr></table></figure>
<h4 id="字符串编码"><a href="#字符串编码" class="headerlink" title="字符串编码"></a>字符串编码</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.ASCII</span><br><span class="line">2.Unicode</span><br><span class="line">3.可变长编码 utf-8</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">文件：test.txt utf-8编码---&gt;(read)读取：转换为unicode编码---&gt;(read)内存中 Unicode编码---&gt;(save)保存：转换为utf-8编码</span><br></pre></td></tr></table></figure>
<h4 id="爬虫去重策略"><a href="#爬虫去重策略" class="headerlink" title="爬虫去重策略"></a>爬虫去重策略</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1.将访问过的url保存到数据库中</span><br><span class="line">2.将访问过的url保存到set中，只需要o(1)的代价就可以查询url 100000000*2byte*50个字符/1024/1024/1024 = 9GB </span><br><span class="line">3.url经过md5等方法哈希后保存到set中</span><br><span class="line">4.用bitmap方法，将访问过的url通过hash函数映射到某一位</span><br><span class="line">5.bloomfilter方法对bitmap进行改进，多重hash函数降低冲突</span><br></pre></td></tr></table></figure>
<h4 id="深度优先和广度优先"><a href="#深度优先和广度优先" class="headerlink" title="深度优先和广度优先"></a>深度优先和广度优先</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">目录</span><br><span class="line">1.网站的树结构</span><br><span class="line">2.深度优先算法和实现</span><br><span class="line">递归实现</span><br><span class="line">3.广度优先算法和实现</span><br><span class="line">队列实现</span><br></pre></td></tr></table></figure>
<h2 id="第4章-scrapy爬取知名技术文章网站"><a href="#第4章-scrapy爬取知名技术文章网站" class="headerlink" title="第4章 scrapy爬取知名技术文章网站"></a>第4章 scrapy爬取知名技术文章网站</h2><h4 id="搭建scrapy的开发环境，本章介绍scrapy的常用命令以及工程目录结构分析，本章中也会详细的讲解xpath和css选择器的使用。然后通过scrapy提供的spider完成所有文章的爬取。然后详细讲解item以及item-loader方式完成具体字段的提取后使用scrapy提供的pipeline分别将数据保存到json文件以及mysql数据库中。…"><a href="#搭建scrapy的开发环境，本章介绍scrapy的常用命令以及工程目录结构分析，本章中也会详细的讲解xpath和css选择器的使用。然后通过scrapy提供的spider完成所有文章的爬取。然后详细讲解item以及item-loader方式完成具体字段的提取后使用scrapy提供的pipeline分别将数据保存到json文件以及mysql数据库中。…" class="headerlink" title="搭建scrapy的开发环境，本章介绍scrapy的常用命令以及工程目录结构分析，本章中也会详细的讲解xpath和css选择器的使用。然后通过scrapy提供的spider完成所有文章的爬取。然后详细讲解item以及item loader方式完成具体字段的提取后使用scrapy提供的pipeline分别将数据保存到json文件以及mysql数据库中。…"></a>搭建scrapy的开发环境，本章介绍scrapy的常用命令以及工程目录结构分析，本章中也会详细的讲解xpath和css选择器的使用。然后通过scrapy提供的spider完成所有文章的爬取。然后详细讲解item以及item loader方式完成具体字段的提取后使用scrapy提供的pipeline分别将数据保存到json文件以及mysql数据库中。…</h4><h4 id="xpath"><a href="#xpath" class="headerlink" title="xpath"></a>xpath</h4><h5 id="xpath简介"><a href="#xpath简介" class="headerlink" title="xpath简介"></a>xpath简介</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.xpath使用路径表达式在xml和html中进行导航</span><br><span class="line">2.xpath包含标准函数库</span><br><span class="line">3.xpath是一个w3c的标准</span><br></pre></td></tr></table></figure>
<h5 id="xpath节点关系"><a href="#xpath节点关系" class="headerlink" title="xpath节点关系"></a>xpath节点关系</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1.父节点</span><br><span class="line">2.子节点</span><br><span class="line">3.同胞节点</span><br><span class="line">4.先辈节点</span><br><span class="line">5.</span><br></pre></td></tr></table></figure>
<h5 id="xpath语法"><a href="#xpath语法" class="headerlink" title="xpath语法"></a>xpath语法</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">表达式 说明</span><br><span class="line">article 选取所有article元素的所有子节点</span><br><span class="line">/article 选取根元素article</span><br><span class="line">article/a 选取所有属于article的子元素的a元素</span><br><span class="line">//div 选取所有div子元素（不论出现在文档任何地方）</span><br><span class="line">article//div 选取所有属于article元素的后代的div元素，不管它出现在article之下的任何位置</span><br><span class="line">//@class 选取所有名为class的属性</span><br><span class="line"></span><br><span class="line">/div/* 选取属于div元素的所有子节点</span><br><span class="line">//* 选取所有元素</span><br><span class="line">//div[@*] 选取所有带属性的title元素</span><br><span class="line">//div/a|//div/p 选取所有div元素的a和p元素</span><br><span class="line">//span|//ul 选取文档中的span和ul元素</span><br><span class="line">article/div/p|//span 选取所有属于article元素的div元素的p元素 以及文档中所有的span元素</span><br></pre></td></tr></table></figure>
<h5 id="xpath语法-谓语"><a href="#xpath语法-谓语" class="headerlink" title="xpath语法-谓语"></a>xpath语法-谓语</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">表达式 说明</span><br><span class="line">/article/div[1] 选取属于article子元素的第一个div元素</span><br><span class="line">/article/div[last()] 选取属于article子元素的最后一个div元素</span><br><span class="line">/article/div[last()-1] 选取属于article子元素的倒数第二个div元素</span><br><span class="line">//div[@lang] 选取所有拥有lang属性的div元素</span><br><span class="line">//div[@lang=&apos;eng&apos;] 选取所有lang属性为eng的div元素</span><br></pre></td></tr></table></figure>
<h5 id="scrapy-shell"><a href="#scrapy-shell" class="headerlink" title="scrapy shell"></a>scrapy shell</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell http://blog.jobbole.com/110287</span><br><span class="line"></span><br><span class="line">title = response.xpath(&quot;//div[@class=&apos;entry-header&apos;]/h1/text()&quot;)</span><br><span class="line"></span><br><span class="line">title</span><br><span class="line"></span><br><span class="line">title.extract()</span><br></pre></td></tr></table></figure>
<h4 id="css选择器"><a href="#css选择器" class="headerlink" title="css选择器"></a>css选择器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">表达式 说明</span><br><span class="line">* 选择所有节点</span><br><span class="line">#container 选择id为container的节点</span><br><span class="line">.container 选择所有class包含container的节点</span><br><span class="line">li a 选择所有li下的所有a节点</span><br><span class="line">ul + p 选择ul后面的第一个p元素</span><br><span class="line">div#container &gt; ul 选取id为container的div的第一个ul子元素</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">表达式 说明</span><br><span class="line">ul ～ p 选取与ul相邻的所有p元素</span><br><span class="line">a[title] 选取所有有title属性的a元素</span><br><span class="line">a[href=&quot;http://jobbole.com&quot;] 选取所有href属性为jobbole.com值的a元素</span><br><span class="line">a[href*=&quot;jobbole&quot;] 选取所有href属性包含jobbole的a元素</span><br><span class="line">a[href^=&quot;http&quot;] 选取所有href属性值以http开头的a元素</span><br><span class="line">a[href$=&quot;.jpg&quot;] 选取所有href属性值以.jpg结尾的a元素</span><br><span class="line">input[type=radio]:checked 选择选中的radio的元素</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">表达式 说明</span><br><span class="line">div:not(#container) 选取所有id非container的div属性</span><br><span class="line">li:nth-child(3) 选取第三个li元素</span><br><span class="line">tr:nth-child(2n) 第偶数个tr</span><br></pre></td></tr></table></figure>
<h5 id="scrapy-shell-1"><a href="#scrapy-shell-1" class="headerlink" title="scrapy shell"></a>scrapy shell</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell http://blog.jobbole.com/110287</span><br><span class="line"></span><br><span class="line">title = response.css(&quot;.entry-header h1::text&quot;)</span><br><span class="line"></span><br><span class="line">title</span><br><span class="line"></span><br><span class="line">title.extract()</span><br></pre></td></tr></table></figure>
<h4 id="scrapy-http"><a href="#scrapy-http" class="headerlink" title="scrapy.http"></a>scrapy.http</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.http import Request</span><br></pre></td></tr></table></figure>
<h4 id="scrapy-djangoitem"><a href="#scrapy-djangoitem" class="headerlink" title="scrapy-djangoitem"></a>scrapy-djangoitem</h4><h4 id="item-loader-jobbole-py"><a href="#item-loader-jobbole-py" class="headerlink" title="item-loader jobbole.py"></a>item-loader jobbole.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line"><span class="comment"># 通过item_loader加载item,JobboleArticleItem()是item.py里的item。</span></span><br><span class="line">item_loader = ItemLoader(item=JobboleArticleItem(), response=response)</span><br><span class="line">item_loader.add_css(<span class="string">"title"</span>, <span class="string">".entry-header h1::text"</span>)</span><br><span class="line">item_loader.add_xpath()</span><br><span class="line">item_loader.add_value(<span class="string">"url"</span>, response.url)</span><br><span class="line"><span class="comment"># 重新加载</span></span><br><span class="line">article_item = item_loader.load_item()</span><br><span class="line"><span class="comment"># 传出</span></span><br><span class="line"><span class="keyword">yield</span> article_item</span><br></pre></td></tr></table></figure>
<h4 id="item-py"><a href="#item-py" class="headerlink" title="item.py"></a>item.py</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">class JobBoleArticleItem(scrapy.Item):</span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    create_date = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(date_convert),</span><br><span class="line">    )</span><br><span class="line">    content = scrapy.Field()</span><br></pre></td></tr></table></figure>
<h2 id="第5章-scrapy爬取知名问答网站"><a href="#第5章-scrapy爬取知名问答网站" class="headerlink" title="第5章 scrapy爬取知名问答网站"></a>第5章 scrapy爬取知名问答网站</h2><hr>
<p>本章主要完成网站的问题和回答的提取。本章除了分析出问答网站的网络请求以外还会分别通过requests和scrapy的FormRequest两种方式完成网站的模拟登录， 本章详细的分析了网站的网络请求并分别分析出了网站问题回答的api请求接口并将数据提取出来后保存到mysql中。</p>
<hr>
<h4 id="cookie"><a href="#cookie" class="headerlink" title="cookie"></a>cookie</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">distributed_crawler_search_engine</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell -s &quot;输入headers&quot;</span><br></pre></td></tr></table></figure>
<p>知乎数据库设计</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">名 类型</span><br><span class="line">zhihu_id bigint 20 0 yes primay key 1</span><br><span class="line">url varchar 300 0 yes</span><br><span class="line">question_id bigint 20 0 yes</span><br><span class="line">author_id varchar 100 0 no</span><br><span class="line">content longtext 0 0 yes</span><br><span class="line">praise_num int 11 0 yes</span><br><span class="line">comments_num int 11 0 yes</span><br><span class="line">create_time date 0 0 yes</span><br><span class="line">update_time date 0 0 yes</span><br><span class="line">crawl_time datetime 0 0 yes</span><br><span class="line">crawl_update_time datetime 0 0 no</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> zhihu_question(<span class="string">`zhihu_id`</span> </span><br><span class="line"> )</span><br></pre></td></tr></table></figure>
<h2 id="第6章-通过CrawlSpider对招聘网站进行整站爬取"><a href="#第6章-通过CrawlSpider对招聘网站进行整站爬取" class="headerlink" title="第6章 通过CrawlSpider对招聘网站进行整站爬取"></a>第6章 通过CrawlSpider对招聘网站进行整站爬取</h2><h4 id="本章完成招聘网站职位的数据表结构设计，并通过link-extractor和rule的形式并配置CrawlSpider完成招聘网站所有职位的爬取，本章也会从源码的角度来分析CrawlSpider让大家对CrawlSpider有深入的理解。"><a href="#本章完成招聘网站职位的数据表结构设计，并通过link-extractor和rule的形式并配置CrawlSpider完成招聘网站所有职位的爬取，本章也会从源码的角度来分析CrawlSpider让大家对CrawlSpider有深入的理解。" class="headerlink" title="本章完成招聘网站职位的数据表结构设计，并通过link extractor和rule的形式并配置CrawlSpider完成招聘网站所有职位的爬取，本章也会从源码的角度来分析CrawlSpider让大家对CrawlSpider有深入的理解。"></a>本章完成招聘网站职位的数据表结构设计，并通过link extractor和rule的形式并配置CrawlSpider完成招聘网站所有职位的爬取，本章也会从源码的角度来分析CrawlSpider让大家对CrawlSpider有深入的理解。</h4><h2 id="第7章-Scrapy突破反爬虫的限制"><a href="#第7章-Scrapy突破反爬虫的限制" class="headerlink" title="第7章 Scrapy突破反爬虫的限制"></a>第7章 Scrapy突破反爬虫的限制</h2><blockquote>
<p>本章会从爬虫和反爬虫的斗争过程开始讲解，然后讲解scrapy的原理，然后通过随机切换user-agent和设置scrapy的ip代理的方式完成突破反爬虫的各种限制。本章也会详细介绍httpresponse和httprequest来详细的分析scrapy的功能，最后会通过云打码平台来完成在线验证码识别以及禁用cookie和访问频率来降低爬虫被屏蔽的可能性。…</p>
</blockquote>
<h3 id="UA-用户代理"><a href="#UA-用户代理" class="headerlink" title="UA 用户代理"></a>UA 用户代理</h3><p>hellysmile/fake-useragent</p>
<h3 id="IP代理"><a href="#IP代理" class="headerlink" title="IP代理"></a>IP代理</h3><p>西刺代理 高匿ip代理<br>aivarsk/scrapy-proxies<br>scrapy-plugins/scrapy-crawlera #收费的<br>Tor</p>
<h3 id="验证码识别"><a href="#验证码识别" class="headerlink" title="验证码识别"></a>验证码识别</h3><p>tesseract-ocr<br>在线打码<br>人工打码<br><a href="http://www.yundama.com" target="_blank" rel="noopener">云打码</a></p>
<p>settings.py<br>禁用cookie<br>自动限速</p>
<p>爬虫内写入 ，自定义设置单个爬虫<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">custom_settings = &#123;</span><br><span class="line">	<span class="string">"COOKIES_ENABLED"</span>: <span class="keyword">True</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="第8章-scrapy进阶开发"><a href="#第8章-scrapy进阶开发" class="headerlink" title="第8章 scrapy进阶开发"></a>第8章 scrapy进阶开发</h2><blockquote>
<p>本章将讲解scrapy的更多高级特性，这些高级特性包括通过selenium和phantomjs实现动态网站数据的爬取以及将这二者集成到scrapy中、scrapy信号、自定义中间件、暂停和启动scrapy爬虫、scrapy的核心api、scrapy的telnet、scrapy的web service和scrapy的log配置和email发送等。 这些特性使得我们不仅只是可以通过scrapy来完成…</p>
</blockquote>
<p>Selenium浏览器自动化测试框架<br>浏览器Drivers</p>
<h2 id="第9章-scrapy-redis分布式爬虫"><a href="#第9章-scrapy-redis分布式爬虫" class="headerlink" title="第9章 scrapy-redis分布式爬虫"></a>第9章 scrapy-redis分布式爬虫</h2><h4 id="Scrapy-redis分布式爬虫的使用以及scrapy-redis的分布式爬虫的源码分析，-让大家可以根据自己的需求来修改源码以满足自己的需求。最后也会讲解如何将bloomfilter集成到scrapy-redis中。"><a href="#Scrapy-redis分布式爬虫的使用以及scrapy-redis的分布式爬虫的源码分析，-让大家可以根据自己的需求来修改源码以满足自己的需求。最后也会讲解如何将bloomfilter集成到scrapy-redis中。" class="headerlink" title="Scrapy-redis分布式爬虫的使用以及scrapy-redis的分布式爬虫的源码分析， 让大家可以根据自己的需求来修改源码以满足自己的需求。最后也会讲解如何将bloomfilter集成到scrapy-redis中。"></a>Scrapy-redis分布式爬虫的使用以及scrapy-redis的分布式爬虫的源码分析， 让大家可以根据自己的需求来修改源码以满足自己的需求。最后也会讲解如何将bloomfilter集成到scrapy-redis中。</h4><h2 id="第10章-elasticsearch搜索引擎的使用"><a href="#第10章-elasticsearch搜索引擎的使用" class="headerlink" title="第10章 elasticsearch搜索引擎的使用"></a>第10章 elasticsearch搜索引擎的使用</h2><h4 id="本章将讲解elasticsearch的安装和使用，将讲解elasticsearch的基本概念的介绍以及api的使用。本章也会讲解搜索引擎的原理并讲解elasticsearch-dsl的使用，最后讲解如何通过scrapy的pipeline将数据保存到elasticsearch中。"><a href="#本章将讲解elasticsearch的安装和使用，将讲解elasticsearch的基本概念的介绍以及api的使用。本章也会讲解搜索引擎的原理并讲解elasticsearch-dsl的使用，最后讲解如何通过scrapy的pipeline将数据保存到elasticsearch中。" class="headerlink" title="本章将讲解elasticsearch的安装和使用，将讲解elasticsearch的基本概念的介绍以及api的使用。本章也会讲解搜索引擎的原理并讲解elasticsearch-dsl的使用，最后讲解如何通过scrapy的pipeline将数据保存到elasticsearch中。"></a>本章将讲解elasticsearch的安装和使用，将讲解elasticsearch的基本概念的介绍以及api的使用。本章也会讲解搜索引擎的原理并讲解elasticsearch-dsl的使用，最后讲解如何通过scrapy的pipeline将数据保存到elasticsearch中。</h4><h2 id="第11章-课程总结"><a href="#第11章-课程总结" class="headerlink" title="第11章 课程总结"></a>第11章 课程总结</h2><h4 id="本章讲解如何通过django快速搭建搜索网站，-本章也会讲解如何完成django与elasticsearch的搜索查询交互。"><a href="#本章讲解如何通过django快速搭建搜索网站，-本章也会讲解如何完成django与elasticsearch的搜索查询交互。" class="headerlink" title="本章讲解如何通过django快速搭建搜索网站， 本章也会讲解如何完成django与elasticsearch的搜索查询交互。"></a>本章讲解如何通过django快速搭建搜索网站， 本章也会讲解如何完成django与elasticsearch的搜索查询交互。</h4><h2 id="第12章-scrapyd部署scrapy爬虫"><a href="#第12章-scrapyd部署scrapy爬虫" class="headerlink" title="第12章 scrapyd部署scrapy爬虫"></a>第12章 scrapyd部署scrapy爬虫</h2><h4 id="本章主要通过scrapyd完成对scrapy爬虫的线上部署。"><a href="#本章主要通过scrapyd完成对scrapy爬虫的线上部署。" class="headerlink" title="本章主要通过scrapyd完成对scrapy爬虫的线上部署。"></a>本章主要通过scrapyd完成对scrapy爬虫的线上部署。</h4><h2 id="第13章-课程总结"><a href="#第13章-课程总结" class="headerlink" title="第13章 课程总结"></a>第13章 课程总结</h2><h4 id="重新梳理一遍系统开发的整个过程，-让同学对系统和开发过程有一个更加直观的理解"><a href="#重新梳理一遍系统开发的整个过程，-让同学对系统和开发过程有一个更加直观的理解" class="headerlink" title="重新梳理一遍系统开发的整个过程， 让同学对系统和开发过程有一个更加直观的理解"></a>重新梳理一遍系统开发的整个过程， 让同学对系统和开发过程有一个更加直观的理解</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install selenium redis elasticsearch_dsl PyMySQL requests</span><br></pre></td></tr></table></figure>

      
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Shell高级编程实战-目录" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/20/Shell高级编程实战-目录/" class="article-date">
      <time datetime="2018-04-20T08:14:15.867Z" itemprop="datePublished">2018-04-20</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="01-Shell高级编程实战（第一、二部）"><a href="#01-Shell高级编程实战（第一、二部）" class="headerlink" title="01.Shell高级编程实战（第一、二部）"></a>01.Shell高级编程实战（第一、二部）</h2><p>01.Shell编程课前思想-我一定要学好Shell编程.mp4 16:28<br>02.学好Shell编程需要的必备基础.mp4 15:03<br>03.Shell脚本介绍及第一个规范Shell脚本说明.mp4 29:00<br>04.Shell编程的作用和地位.mp4 05:59<br>05.Shell语言的种类介绍.mp4 05:03<br>06.Shell的条件表达式知识初步介绍实践.mp4 22:50<br>07.Shell的字符串表达式介绍-实践及企业案例脚本剖析.mp4 16:09<br>08.Shell的整数表达式介绍-实践及企业案例脚本剖析.mp4 14:39<br>09.Shell的逻辑操作符知识介绍-实践及企业案例脚本剖析.mp4 11:10<br>10.Shell的各种表达式综合脚本开发实战讲解.mp4 20:29<br>11.利用所学知识打印一二级菜单操作企业案例.mp4 17:48<br>12.if条件句语法介绍及形象比喻.mp4 12:01<br>13.监控系统内存并报警企业案例脚本开发实战.mp4 24:13<br>14.利用if监控web和db的多种方法介绍及实践1.mp4 21:02<br>15.利用if监控web和db的多种方法介绍及实践2.mp4 44:00<br>16.Shell课堂学生拿到OFFER分享.mp4 16:06<br>17.Shell分组学习规划.mp4 22:02<br>18.课后作业.mp4 02:50</p>
<h2 id="02-Shell高级编程实战（第三、四部）"><a href="#02-Shell高级编程实战（第三、四部）" class="headerlink" title="02.Shell高级编程实战（第三、四部）"></a>02.Shell高级编程实战（第三、四部）</h2><p>01.课前提问上节知识与内容回顾.mp4 16:23<br>02.Shell的数值运算深度实践4.mp4<br>03.Shell的数值运算知识深度实践5.mp4<br>04.变量的读入之read知识及实战脚本开发.mp4<br>05.Shell的条件表达式知识初步介绍实践.mp4<br>06.Shell的条件表达式知识初步介绍实践.mp4<br>07.Shell的字符串表达式介绍-实践及企业案例脚本剖析.mp4<br>08.Shell的整数表达式介绍-实践及企业案例脚本剖析.mp4<br>09.Shell的逻辑操作符知识介绍-实践及企业案例脚本剖析.mp4<br>10.Shell的各种表达式综合脚本开发实战讲解.mp4<br>11.利用所学知识打印一二级菜单操作企业案例.mp4<br>12.if条件句语法介绍及形象比喻.mp4<br>13.监控系统内存并报警企业案例脚本开发实战.mp4<br>14.利用if监控web和db的多种方法介绍及实践1.mp4<br>15.利用if监控web和db的多种方法介绍及实践2.mp4<br>16.Shell课堂学生拿到OFFER分享.mp4<br>17.Shell分组学习规划.mp4<br>18.课后作业.mp4</p>
<h2 id="03-Shell高级编程实战（第五、六部）"><a href="#03-Shell高级编程实战（第五、六部）" class="headerlink" title="03.Shell高级编程实战（第五、六部）"></a>03.Shell高级编程实战（第五、六部）</h2><p>01.课前思想-葛亮挥泪斩马谡.mp4 19:34<br>02.企业级增删改查用户脚本开发实战分享.mp4<br>03.Shell函数的介绍及实践.mp4<br>04.Shell的函数及函数传参实践.mp4<br>05.监控网站是否异常企业案例脚本开发手把手讲解.mp4<br>06.给任意字符串加指定颜色的企业级函数案例实战.mp4<br>07.shell的case语句介绍及案例实践.mp4<br>08.开发Nginx启动脚本并实现加入开机自启动管理实战.mp4<br>09.开发Nginx启动脚本并实现加入开机自启动管理实战-老男孩点评修改.mp4<br>10.case语句企业案例和系统脚本剖析.mp4<br>11.while循环介绍及实践.mp4<br>12.Shell脚本手机充值案例精讲.mp4<br>13.Shell脚本手机充值案例精讲后修改及点评.mp4<br>14.While循环读取日志分析的企业案例多种方法精讲.mp4<br>15.课后作业与下节内容预习说明.mp4</p>
<h2 id="04-Shell高级编程实战（第七、八部）"><a href="#04-Shell高级编程实战（第七、八部）" class="headerlink" title="04.Shell高级编程实战（第七、八部）"></a>04.Shell高级编程实战（第七、八部）</h2><h2 id="05-Shell高级编程实战（第九、十部）"><a href="#05-Shell高级编程实战（第九、十部）" class="headerlink" title="05.Shell高级编程实战（第九、十部）"></a>05.Shell高级编程实战（第九、十部）</h2><h2 id="06-Shell高级编程实战（第十一、十二部）"><a href="#06-Shell高级编程实战（第十一、十二部）" class="headerlink" title="06.Shell高级编程实战（第十一、十二部）"></a>06.Shell高级编程实战（第十一、十二部）</h2><h2 id="07-Shell高级编程实战（第十三部）三剑客之sed实践讲解"><a href="#07-Shell高级编程实战（第十三部）三剑客之sed实践讲解" class="headerlink" title="07.Shell高级编程实战（第十三部）三剑客之sed实践讲解"></a>07.Shell高级编程实战（第十三部）三剑客之sed实践讲解</h2><h2 id="08-Shell高级编程实战（第十四部）AWK数组国内企业案例"><a href="#08-Shell高级编程实战（第十四部）AWK数组国内企业案例" class="headerlink" title="08.Shell高级编程实战（第十四部）AWK数组国内企业案例"></a>08.Shell高级编程实战（第十四部）AWK数组国内企业案例</h2>
      
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/7/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="page-number current">8</span><a class="page-number" href="/page/9/">9</a><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/30/">30</a><a class="extend next" rel="next" href="/page/9/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                &copy; 2018 幻舞梦境
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/luuman/hexo-theme-spfk" target="_blank">spfk</a> by luuman
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" >海贼到访数: 
                            <span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>, </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit">本页阅读量: 
                            <span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>

    </div>
    <script src="https://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
<script src="/js/main.js"></script>

    <script>
        $(document).ready(function() {
            var backgroundnum = 24;
            var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
            $("#mobile-nav").css({"background-image": backgroundimg,"background-size": "cover","background-position": "center"});
            $(".left-col").css({"background-image": backgroundimg,"background-size": "cover","background-position": "center"});
        })
    </script>


<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-39498261-3', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->



	<script>
	var _hmt = _hmt || [];
	(function() {
	  var hm = document.createElement("script");
	  hm.src = "//hm.baidu.com/hm.js?1bb44bf25fea8f000a1397f9b5438de7";
	  var s = document.getElementsByTagName("script")[0]; 
	  s.parentNode.insertBefore(hm, s);
	})();
	</script>


<div class="scroll" id="scroll">
    <a href="#"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments"><i class="fa fa-comments-o"></i></a>
    <a href="#footer"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    $(document).ready(function() {
        if ($("#comments").length < 1) {
            $("#scroll > a:nth-child(2)").hide();
        };
    })
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

  <script language="javascript">
    $(function() {
        $("a[title]").each(function() {
            var a = $(this);
            var title = a.attr('title');
            if (title == undefined || title == "") return;
            a.data('title', title).removeAttr('title').hover(

            function() {
                var offset = a.offset();
                $("<div id=\"anchortitlecontainer\"></div>").appendTo($("body")).html(title).css({
                    top: offset.top - a.outerHeight() - 15,
                    left: offset.left + a.outerWidth()/2 + 1
                }).fadeIn(function() {
                    var pop = $(this);
                    setTimeout(function() {
                        pop.remove();
                    }, pop.text().length * 800);
                });
            }, function() {
                $("#anchortitlecontainer").remove();
            });
        });
    });
</script>


  </div>
</body>
</html>