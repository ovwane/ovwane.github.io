<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>幻舞梦境</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="幻舞梦境">
<meta property="og:url" content="http://ovwane.me/page/11/index.html">
<meta property="og:site_name" content="幻舞梦境">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="幻舞梦境">
  
    <link rel="alternative" href="/atom.xml" title="幻舞梦境" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/favicon.png">
  
  
      <link rel="stylesheet" href="//cdn.bootcss.com/animate.css/3.5.0/animate.min.css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/font-awesome/css/font-awesome.min.css">
  <link rel="apple-touch-icon" href="/apple-touch-icon.png">
  
     
      <meta name="baidu-site-verification" content="3rC08I0Cp6" />
    
     
      <meta name="google-site-verification" content="rn8f25RzFQoiByt8ezg9E17KZIElbIlwJ4y-EKSlWbA" />
    
  
  
      <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  
  <!-- 加载特效 -->
    <script src="/js/pace.js"></script>
    <link href="/css/pace/pace-theme-flash.css" rel="stylesheet" />
  <script>
      var yiliaConfig = {
          rootUrl: '/',
          fancybox: true,
          animate: true,
          isHome: true,
          isPost: false,
          isArchive: false,
          isTag: false,
          isCategory: false,
          open_in_new: false
      }
  </script>
</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            
            <img lazy-src="/img/head.jpg" class="js-avatar">
            
        </a>

        <hgroup>
          <h1 class="header-author"><a href="/" title="Hi Mate">幻舞梦境</a></h1>
        </hgroup>

        
        
        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">首页</a></li>
                        
                            <li><a href="/WebGuide">导航</a></li>
                        
                            <li><a href="/ovwane_love">Love</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fl github" target="_blank" href="https://github.com/ovwane" title="github">github</a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <a href="/tags/Ansible/" style="font-size: 11.67px;">Ansible</a> <a href="/tags/Bind/" style="font-size: 10px;">Bind</a> <a href="/tags/Cacti/" style="font-size: 10px;">Cacti</a> <a href="/tags/CentOS/" style="font-size: 20px;">CentOS</a> <a href="/tags/Cobbler/" style="font-size: 10px;">Cobbler</a> <a href="/tags/DHCP/" style="font-size: 10px;">DHCP</a> <a href="/tags/Django/" style="font-size: 13.33px;">Django</a> <a href="/tags/Docker/" style="font-size: 10px;">Docker</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Gitlab/" style="font-size: 11.67px;">Gitlab</a> <a href="/tags/HAProxy/" style="font-size: 10px;">HAProxy</a> <a href="/tags/Hexo/" style="font-size: 11.67px;">Hexo</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Jenkins/" style="font-size: 11.67px;">Jenkins</a> <a href="/tags/KVM/" style="font-size: 10px;">KVM</a> <a href="/tags/Kubernetes/" style="font-size: 10px;">Kubernetes</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/MariaDB/" style="font-size: 10px;">MariaDB</a> <a href="/tags/MySQL/" style="font-size: 10px;">MySQL</a> <a href="/tags/NFS/" style="font-size: 10px;">NFS</a> <a href="/tags/Nagios/" style="font-size: 10px;">Nagios</a> <a href="/tags/Nginx/" style="font-size: 16.67px;">Nginx</a> <a href="/tags/Oh-My-Zsh/" style="font-size: 10px;">Oh My Zsh</a> <a href="/tags/OpenLDAP/" style="font-size: 10px;">OpenLDAP</a> <a href="/tags/OpenStack/" style="font-size: 10px;">OpenStack</a> <a href="/tags/PHP/" style="font-size: 11.67px;">PHP</a> <a href="/tags/PXE/" style="font-size: 10px;">PXE</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/RHEL7/" style="font-size: 10px;">RHEL7</a> <a href="/tags/Redis/" style="font-size: 10px;">Redis</a> <a href="/tags/Requests/" style="font-size: 10px;">Requests</a> <a href="/tags/Resin/" style="font-size: 10px;">Resin</a> <a href="/tags/Samba/" style="font-size: 10px;">Samba</a> <a href="/tags/Scrapy/" style="font-size: 11.67px;">Scrapy</a> <a href="/tags/Scrapyd/" style="font-size: 10px;">Scrapyd</a> <a href="/tags/SpiderKeeper/" style="font-size: 10px;">SpiderKeeper</a> <a href="/tags/Squid/" style="font-size: 10px;">Squid</a> <a href="/tags/Subversion/" style="font-size: 10px;">Subversion</a> <a href="/tags/Supervisor/" style="font-size: 10px;">Supervisor</a> <a href="/tags/Tomcat/" style="font-size: 13.33px;">Tomcat</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/Web/" style="font-size: 10px;">Web</a> <a href="/tags/Xadmin/" style="font-size: 10px;">Xadmin</a> <a href="/tags/Zabbix/" style="font-size: 10px;">Zabbix</a> <a href="/tags/cacti/" style="font-size: 10px;">cacti</a> <a href="/tags/dovecot/" style="font-size: 10px;">dovecot</a> <a href="/tags/httpd/" style="font-size: 11.67px;">httpd</a> <a href="/tags/iSCSI/" style="font-size: 10px;">iSCSI</a> <a href="/tags/k8s/" style="font-size: 10px;">k8s</a> <a href="/tags/kickstart/" style="font-size: 10px;">kickstart</a> <a href="/tags/macOS/" style="font-size: 15px;">macOS</a> <a href="/tags/nagios/" style="font-size: 10px;">nagios</a> <a href="/tags/postfix/" style="font-size: 10px;">postfix</a> <a href="/tags/rsync/" style="font-size: 10px;">rsync</a> <a href="/tags/sshd/" style="font-size: 10px;">sshd</a> <a href="/tags/uWSGI/" style="font-size: 10px;">uWSGI</a> <a href="/tags/vsftpd/" style="font-size: 10px;">vsftpd</a> <a href="/tags/zabbix/" style="font-size: 10px;">zabbix</a> <a href="/tags/历史/" style="font-size: 18.33px;">历史</a> <a href="/tags/情感/" style="font-size: 10px;">情感</a> <a href="/tags/生活/" style="font-size: 10px;">生活</a> <a href="/tags/监控/" style="font-size: 13.33px;">监控</a> <a href="/tags/自己/" style="font-size: 10px;">自己</a> <a href="/tags/运维/" style="font-size: 10px;">运维</a>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a target="_blank" class="main-nav-link switch-friends-link" href="http://luuman.github.io/">name</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">纯海迷、爱运动、爱交友、爱旅行、喜欢接触新鲜事物、迎接新的挑战，更爱游离于错综复杂的编码与逻辑中</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="Me">幻舞梦境</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                
                    <img lazy-src="/img/head.jpg" class="js-avatar">
                
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="Me">幻舞梦境</a></h1>
            </hgroup>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">首页</a></li>
                
                    <li><a href="/WebGuide">导航</a></li>
                
                    <li><a href="/ovwane_love">Love</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                <div class="social">
                    
                        <a class="github" target="_blank" href="https://github.com/ovwane" title="github">github</a>
                    
                </div>
            </nav>
        </header>                
    </div>
</nav>
      <div class="body-wrap">
  
    <article id="post-Python自动化开发" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/07/24/Python自动化开发/" class="article-date">
      <time datetime="2018-07-24T01:20:41.417Z" itemprop="datePublished">2018-07-24</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p><a href="http://www.oldboyedu.com/course/weekend/pythonzdh/" target="_blank" rel="noopener">老男孩Python自动化开发</a></p>
<p><a href="http://edu.csdn.net/lecturer/491?type=1&amp;page=5#content" target="_blank" rel="noopener">Python自动化开发基础</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1.Python语言基础（基本数据类型、函数、模块、面向对象）</span><br><span class="line">2.网络编程以及并发（Socket、线程进程、IO多路复用）</span><br><span class="line">3.前端开发（HTML、CSS、JavaScript、jQuery、BootStrap）</span><br><span class="line">4.WEB框架（MVC、MTV、ORM、自定义组件）</span><br><span class="line">5.企业项目实战（Web QQ， CRM系统，CRM系统开发，金融量化分析交易系统）</span><br><span class="line">6.企业项目实战视频 （45课时视频赠送）</span><br><span class="line">7.算法、数据结构、设计模式（20课时视频赠送）</span><br></pre></td></tr></table></figure>
<p>全部课程44周，每周1天，每天8个小时。</p>
<h2 id="Python自动化开发精英班-23周"><a href="#Python自动化开发精英班-23周" class="headerlink" title="Python自动化开发精英班 23周"></a>Python自动化开发精英班 23周</h2><h3 id="1-Python基础-3周"><a href="#1-Python基础-3周" class="headerlink" title="1.Python基础 3周"></a>1.Python基础 3周</h3><ol>
<li>Python简介</li>
<li>与其他语言对比</li>
<li>字符编码/注释/变量/缩进</li>
<li>流程控制</li>
<li>常用数据介绍</li>
<li>数据类型内置方法</li>
<li>字符串格式化</li>
<li>运算符</li>
<li>输入输出</li>
<li>三元运算</li>
<li>collections</li>
<li>列表、字典、元组、集合详细使用</li>
<li>IO操作，文件增删改查</li>
<li>函数介绍，函数语法，函数参数</li>
<li>高阶函数与函数返回值</li>
<li>lambdb匿名函数</li>
</ol>
<h3 id="2-函数和常用模块-2周"><a href="#2-函数和常用模块-2周" class="headerlink" title="2.函数和常用模块 2周"></a>2.函数和常用模块 2周</h3><ol>
<li>装饰器 原理与使用</li>
<li>高阶函数</li>
<li>迭代器与生成器</li>
<li>函数式编程，函数递归</li>
<li>二分查找</li>
<li>模块介绍</li>
<li>re 正则模块</li>
<li>os\sys 模块</li>
<li>subprocess 模块</li>
<li>shutil\shelve\itertools 模块</li>
<li>hashlib 模块</li>
<li>logging 日志模块</li>
<li>time\datetime\traceback模块</li>
<li>json\pickle\xml\yaml\configparser 模块</li>
<li>urllib\paramiko 模块</li>
</ol>
<h3 id="3-面向对象-2周"><a href="#3-面向对象-2周" class="headerlink" title="3.面向对象 2周"></a>3.面向对象 2周</h3><ol>
<li>面向对象简介</li>
<li>类肯对象是什么</li>
<li>创建类</li>
<li>经典类与新式类</li>
<li>面向对象三大特性：封装、继承、多态</li>
<li>类的普通成员</li>
<li>字段方法属性</li>
<li>类的高级成员</li>
<li>静态字段、静态方法、静态属性</li>
<li>类方法</li>
<li>MetaClass\AbstractClass</li>
<li>类成员修饰符</li>
<li>函数式编程和面向对象编程的对比</li>
<li>扩展：面向对象编程的本质</li>
<li>网络编程基础</li>
<li>socket原理讲解</li>
<li>通过socket实现简单ssh</li>
</ol>
<h3 id="04-网络编程基础-2周"><a href="#04-网络编程基础-2周" class="headerlink" title="04.网络编程基础 2周"></a>04.网络编程基础 2周</h3><ol>
<li>Python模块socketserver使用和源码析（异步多线程）</li>
<li>线程、进程、协程原理析</li>
<li>线程相关各知识点详解</li>
<li>Python中线程和其它语言线程的对比</li>
<li>GIL内部机制</li>
<li>线程锁、事件</li>
<li>生产者消费者模型（消息队列）</li>
<li>进程的使用</li>
<li>进程间数据共享</li>
<li>进程池</li>
</ol>
<h3 id="05-数据库、缓存、队列-2周"><a href="#05-数据库、缓存、队列-2周" class="headerlink" title="05.数据库、缓存、队列 2周"></a>05.数据库、缓存、队列 2周</h3><ol>
<li>Python操作Redis</li>
<li>Python操作Memcached</li>
<li>RabbitMQ消息队列</li>
<li>数据库介绍</li>
<li>MySQL数据库安装使用</li>
<li>MySQL管理</li>
<li>MySQL数据类型</li>
<li>常用MySQL命令</li>
<li>创建数据库</li>
<li>外键</li>
<li>增删改查表</li>
<li>权限</li>
<li>事务</li>
<li>索引</li>
<li>Python操作MySQL</li>
</ol>
<h3 id="06-WEB开发基础-3周"><a href="#06-WEB开发基础-3周" class="headerlink" title="06.WEB开发基础 3周"></a>06.WEB开发基础 3周</h3><ol>
<li>HTML基础</li>
<li>CSS基础</li>
<li>JavaScript基础</li>
<li>局部变量和全局变量</li>
<li>集合、数组和字典</li>
<li>函数参数</li>
<li>原型、面向对象</li>
<li>作用域</li>
<li>dom编程</li>
<li>jQuery介绍、jQuery选择器</li>
<li>jQuery属性和CSS操作</li>
<li>jQuery文档处理</li>
<li>jQuery筛选</li>
<li>jQuery事件托管</li>
<li>jQuery ajax</li>
<li>jQuery扩展方法</li>
<li>Bootstrap使用</li>
<li>EasyUI介绍和使用</li>
</ol>
<h3 id="07-Web框架学习-3周"><a href="#07-Web框架学习-3周" class="headerlink" title="07.Web框架学习 3周"></a>07.Web框架学习 3周</h3><ol>
<li>Web框架本质</li>
<li>Socket服务器</li>
<li>基于反射的路由系统</li>
<li>WSGI介绍及原理实现</li>
<li>开发自己的Web框架</li>
<li>MVC和MTV</li>
<li>路由系统、模板</li>
<li>实现登录、注册、找回密码</li>
<li>Django基础学习与使用</li>
<li>普通路由和动态路由</li>
<li>模板引擎、ORM介绍</li>
<li>Django ORM 增删改查学习</li>
<li>初识自定义tag</li>
<li>Django进阶学习与使用</li>
<li>初识模型绑定、初始Form表单验证</li>
<li>Django ORM 进阶学习</li>
<li>ModelForm、自定义Validator</li>
<li>项目实战：BBS论坛开发</li>
</ol>
<h3 id="08-企业项目实战-6周"><a href="#08-企业项目实战-6周" class="headerlink" title="08.企业项目实战 6周"></a>08.企业项目实战 6周</h3><ol>
<li>自定义tag</li>
<li>模型绑定</li>
<li>Form表单验证</li>
<li>Django admin使用与定制</li>
<li>XXS、CSRF、Session\Cookie</li>
<li>项目实战：Web QQ开发</li>
<li>项目实战：CRM客户管理系统开发</li>
<li>项目实战：爬虫开发</li>
<li>项目实战：金融量化交易策略分析系统</li>
</ol>
<h2 id="Python自动化资深架构师班大纲-共21周"><a href="#Python自动化资深架构师班大纲-共21周" class="headerlink" title="Python自动化资深架构师班大纲 共21周"></a>Python自动化资深架构师班大纲 共21周</h2><h3 id="01-项目实战开发基础-3周"><a href="#01-项目实战开发基础-3周" class="headerlink" title="01.项目实战开发基础 3周"></a>01.项目实战开发基础 3周</h3><ol>
<li>线程、进程和协程</li>
<li>IO多路复用原理以及应用</li>
<li>异步IO模块使用以及高级定制</li>
<li>Twisted框架以及源码析</li>
<li>MySQL基础以及优化</li>
<li>基于Python实现数据库连接池</li>
<li>ORM框架SqlAlchemy</li>
<li>Celery使用以及和Django结合</li>
<li>协同开发之Git全套</li>
<li>软件团队工作流程</li>
<li>敏捷开发与持续集成介绍</li>
</ol>
<h3 id="02-项目实战-爬虫开发-1周"><a href="#02-项目实战-爬虫开发-1周" class="headerlink" title="02.项目实战-爬虫开发 1周"></a>02.项目实战-爬虫开发 1周</h3><ol>
<li>Requests模块</li>
<li>BeautifulSoup模块</li>
<li>基于Requests实现登录：抽屉、github、知乎、博客园</li>
<li>开发Web微信</li>
<li>IO性能相关模块</li>
<li>自定义异步非阻塞模块</li>
<li>Scrapy框架以及源码析</li>
</ol>
<h3 id="03-实战项目-IT审计系统-主机管理开发-2周"><a href="#03-实战项目-IT审计系统-主机管理开发-2周" class="headerlink" title="03.实战项目-IT审计系统+主机管理开发 2周"></a>03.实战项目-IT审计系统+主机管理开发 2周</h3><ol>
<li>用户行为审计</li>
<li>基于底层SSH</li>
<li>主机分组管理</li>
<li>服务器和账号的操作权限</li>
<li>记录堡垒所有操作日志</li>
<li>批量分发操作并查看结果</li>
<li>指定时间单次或重复执行指定的任务</li>
<li>查看近期用户行为报表</li>
<li>提供Web页面SSH操作</li>
</ol>
<h3 id="04-项目实战-CMDB开发-2周"><a href="#04-项目实战-CMDB开发-2周" class="headerlink" title="04.项目实战-CMDB开发 2周"></a>04.项目实战-CMDB开发 2周</h3><ol>
<li>设计符合企业实际需求的CMDB表结构</li>
<li>安全API接口开发认证</li>
<li>开发支持Windows和Linux平台</li>
<li>实现IT硬件信息的自动收集以及变更记录</li>
<li>对其他系统开发灵活的API</li>
<li>设计与开发IT资产变更流程</li>
<li>开发报表功能</li>
</ol>
<h3 id="05-实战项目-金融量化交易策略分析系统-1周"><a href="#05-实战项目-金融量化交易策略分析系统-1周" class="headerlink" title="05.实战项目-金融量化交易策略分析系统 1周"></a>05.实战项目-金融量化交易策略分析系统 1周</h3><ol>
<li>股票、期货基础知识介绍</li>
<li>Numpy、Pandas、Scipy等模块学习</li>
<li>策略平台的介绍与使用</li>
<li>调取股票市场数据API学习</li>
<li>常见金融分析策略，如：双均线、羊驼策略等</li>
<li>编写自定制的量化交易策略</li>
<li>量化交易策略进行回测</li>
<li>通过”双均线”策略进行交易策略开发及回测</li>
</ol>
<h3 id="06-实战项目-分布式监控系统开发-2周"><a href="#06-实战项目-分布式监控系统开发-2周" class="headerlink" title="06.实战项目-分布式监控系统开发 2周"></a>06.实战项目-分布式监控系统开发 2周</h3><ol>
<li>同一台主机监控多个服务</li>
<li>监控多种设备</li>
<li>批量监控主机</li>
<li>设定告警级别</li>
<li>不同服务设定不同告警级别</li>
<li>不同告警 发送不同用户</li>
<li>告警升级</li>
<li>告警的自动恢复</li>
<li>长期监控数据存储的优化</li>
<li>监控数据前端画图实现</li>
</ol>
<h3 id="07-实战项目-网站用户访问质量分析检测项目-1周"><a href="#07-实战项目-网站用户访问质量分析检测项目-1周" class="headerlink" title="07.实战项目-网站用户访问质量分析检测项目 1周"></a>07.实战项目-网站用户访问质量分析检测项目 1周</h3><ol>
<li>实现分析数据的前端美观展示</li>
<li>用户上网指标收集</li>
<li>不同维度用户数据的自动分析并生成报表</li>
<li>最慢排名</li>
<li>速度区间划分</li>
<li>实时分析访问速度及访问数据</li>
<li>统计分析多个网站</li>
</ol>
<h3 id="08-实战项目-Docker自动化管理平台开发-2周"><a href="#08-实战项目-Docker自动化管理平台开发-2周" class="headerlink" title="08.实战项目-Docker自动化管理平台开发 2周"></a>08.实战项目-Docker自动化管理平台开发 2周</h3><ol>
<li>Docker实现原理介绍</li>
<li>Docker安装使用</li>
<li>Docker各组件介绍</li>
<li>通过API管理Docker镜像库、Docker</li>
<li>container</li>
<li>通过Web管理Docker镜像库、Docker</li>
<li>实现对Docker集群的管理</li>
</ol>
<h3 id="09-实战项目-OpenStak私有云平台开发-2周"><a href="#09-实战项目-OpenStak私有云平台开发-2周" class="headerlink" title="09.实战项目-OpenStak私有云平台开发 2周"></a>09.实战项目-OpenStak私有云平台开发 2周</h3><ol>
<li>OpenStack各组件讲解</li>
<li>OpenStack各组件源码分析</li>
<li>OpenStack云平台架构</li>
<li>ceph存储</li>
<li>OpenStack云平台使用手册</li>
<li>OpenStack运维工具开发</li>
<li>OpenStack源码二次开发</li>
</ol>
<h3 id="10-算法、数据结构与设计模式-2周"><a href="#10-算法、数据结构与设计模式-2周" class="headerlink" title="10.算法、数据结构与设计模式 2周"></a>10.算法、数据结构与设计模式 2周</h3><ol>
<li>常用算法介绍</li>
<li>时间/空间复杂度介绍</li>
<li>二分查找算法</li>
<li>哈希算法，冒泡排序</li>
<li>快速排序，直接插入排序</li>
<li>选择排序，二叉树特点及类型介绍</li>
<li>堆、栈、树及其他常用数据结构学习</li>
<li>平衡树/红黑树，堆排序</li>
<li>设计模式简介</li>
<li>简单工厂模式，工厂方法模式</li>
<li>抽象工厂模式</li>
<li>策略模式，装饰模式</li>
<li>代理模式，原型模式</li>
<li>模板方法模式，其他设计模式</li>
</ol>
<h3 id="11-项目实战-Tornao-Web框架学习-amp-源码析-1周"><a href="#11-项目实战-Tornao-Web框架学习-amp-源码析-1周" class="headerlink" title="11.项目实战-Tornao Web框架学习&amp;源码析 1周"></a>11.项目实战-Tornao Web框架学习&amp;源码析 1周</h3><ol>
<li>快速上手</li>
<li>路由系统</li>
<li>RequestHandler</li>
<li>模板，Cookie和安全Cookie</li>
<li>跨站伪造请求的防范</li>
<li>ORM框架SQLAlchemy</li>
<li>静态文件和主动式文件缓存</li>
<li>本地化，非阻塞式异步请求</li>
<li>源码析</li>
<li>一个脚本引发的血案</li>
<li>待请求阶段，请求来了</li>
<li>模板语言</li>
<li>Flask框架讲解</li>
<li>Web.py框架讲解</li>
<li>Bottle框架讲解</li>
</ol>
<h3 id="12-实战项目-机器学习-人工智能-2周"><a href="#12-实战项目-机器学习-人工智能-2周" class="headerlink" title="12.实战项目-机器学习+人工智能 2周"></a>12.实战项目-机器学习+人工智能 2周</h3><ol>
<li>机器学习的基本概念、常见流派及几大要素介绍</li>
<li>介绍训练集、测试集、评价标准</li>
<li>介绍分类、聚类、回归、神经网络</li>
<li>Python机器学习常用库scikit-learn介绍</li>
<li>实现一个简单的分类器算法</li>
<li>机器学习主要流程1：数据预处理</li>
<li>机器学习主要流程2：特征选择、模型选择</li>
<li>机器学习主要流程3：建立模型</li>
<li>机器学习主要流程4：训练模型</li>
<li>机器学习主要流程5：模型评估与调参</li>
<li>神经网络介绍</li>
</ol>

      
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Python相关的书签" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/07/24/Python相关的书签/" class="article-date">
      <time datetime="2018-07-24T01:20:41.417Z" itemprop="datePublished">2018-07-24</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="求职"><a href="#求职" class="headerlink" title="求职"></a>求职</h1><p><a href="https://groups.google.com/forum/m/#!topic/python-cn/JN_Q9o4t8ZM" target="_blank" rel="noopener">新人到底需要什么?</a></p>
<p><a href="https://www.zhihu.com/question/61103114" target="_blank" rel="noopener">Python 爬虫学到什么样就可以找工作了？</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/30518284" target="_blank" rel="noopener">我是如何从自学编程到找到工作的</a></p>
<p><a href="https://www.zhihu.com/question/29775447" target="_blank" rel="noopener">有多少人按@萧井陌大神给出的Python+Flask路线找到工作了？</a></p>
<p>现在看的爬虫代码，学会了就可以找工作了</p>
<h3 id="Python爬虫面试题"><a href="#Python爬虫面试题" class="headerlink" title="Python爬虫面试题"></a>Python爬虫面试题</h3><p><a href="https://github.com/taizilongxu/interview_python" target="_blank" rel="noopener">关于Python的面试题</a></p>
<p><a href="https://www.cnblogs.com/super-super-/p/7545952.html" target="_blank" rel="noopener">Python爬虫面试（2017.09.18）</a></p>
<p><a href="http://www.cnblogs.com/skiler/category/1008825.html" target="_blank" rel="noopener">Python面试与分析</a></p>
<p><a href="http://www.lqkweb.com/blog.php?id=91" target="_blank" rel="noopener">python工程师四家公司面试题</a></p>
<p><a href="https://halshaw.github.io/2016/11/13/%E6%88%91%E7%9A%84%E9%9D%A2%E8%AF%95%E4%B9%8B%E6%97%85/" target="_blank" rel="noopener">我的面试之旅</a></p>
<p><a href="https://www.zhihu.com/collection/160863281" target="_blank" rel="noopener">python爬虫相关</a></p>
<p><a href="http://blog.csdn.net/YLBF_DEV/article/details/51479449" target="_blank" rel="noopener">Python 爬虫的工具列表( 附Github代码下载链接)</a></p>
<p><a href="https://github.com/lining0806/PythonSpiderNotes" target="_blank" rel="noopener">Python入门网络爬虫之精华版</a></p>
<p><a href="https://github.com/shfanzie/Systematically_self-study_Python" target="_blank" rel="noopener">Systematically_self-study_Python</a></p>
<p><a href="https://github.com/taizilongxu" target="_blank" rel="noopener">hackerxu</a></p>
<h1 id="博客"><a href="#博客" class="headerlink" title="博客"></a>博客</h1><h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><p>胡阳<br><a href="https://www.the5fire.com/" target="_blank" rel="noopener">the5fire的技术博客</a></p>
<p>廖雪峰<br><a href="https://www.liaoxuefeng.com/" target="_blank" rel="noopener">廖雪峰的官方网站</a></p>
<p>董伟明<br><a href="http://www.dongwm.com/" target="_blank" rel="noopener">小明明s à domicile | Python之美</a></p>
<p>Alex Li<br><a href="http://www.cnblogs.com/alex3714/articles/5885096.html" target="_blank" rel="noopener">金角大王等待唐僧的日子-老男孩教育Python自动化2.0课程课件目录</a></p>
<p>零度<br><a href="https://nyloner.cn/" target="_blank" rel="noopener">零度</a><br><a href="https://nyloner.cn/categorys?key=%E7%88%AC%E8%99%AB" target="_blank" rel="noopener">零度-文章-爬虫</a></p>
<p><a href="http://ictar.github.io/" target="_blank" rel="noopener">Ele - A面</a></p>
<h2 id="python爬虫-amp-amp-数据挖掘"><a href="#python爬虫-amp-amp-数据挖掘" class="headerlink" title="python爬虫&amp;&amp;数据挖掘"></a>python爬虫&amp;&amp;数据挖掘</h2><p>十四君<br><a href="https://www.urlteam.org/category/web_crawlers/" target="_blank" rel="noopener">URl-team python爬虫&amp;&amp;数据挖掘</a></p>
<p>崔庆才<br><a href="https://cuiqingcai.com/" target="_blank" rel="noopener">静觅 崔庆才的个人博客</a><br><a href="https://ask.hellobi.com/blog/cuiqingcai" target="_blank" rel="noopener">崔庆才的个人博客-hellobi</a></p>
<p>州的先生<br><a href="http://zmister.com/" target="_blank" rel="noopener">州的先生</a></p>
<p>Ehco Blog<br><a href="http://www.ehcoblog.ml/" target="_blank" rel="noopener">Ehco Blog</a></p>
<p>Authors: A. Jesse Jiryu Davis and Guido van Rossum<br><a href="https://github.com/aosabook/500lines/tree/master/crawler" target="_blank" rel="noopener">aosabook/500lines</a></p>
<h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><p><a href="http://www.lining0806.com/" target="_blank" rel="noopener">宁哥的小站</a></p>
<p>#HTTP<br>HTTP POST参数</p>
<p><a href="https://www.zhihu.com/question/60256922/answer/174211193" target="_blank" rel="noopener">Python爬虫传送post请求要携带哪些参数?</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/27097051" target="_blank" rel="noopener">60秒GET小技能-爬虫快速构建post参数法</a></p>
<p>#破解<br><a href="blog.csdn.net/paololiu/article/details/52514504">破解极验(geetest)验证码 - CSDN博客</a></p>
<p>#数据可视化<br><a href="https://www.jianshu.com/p/19f1e63bfe2b" target="_blank" rel="noopener">有趣的数据可视化 - 简书</a></p>
<p>#人工<br>SnailTyan<br><a href="http://noahsnail.com/" target="_blank" rel="noopener">SnailTyan</a></p>
<h1 id="Python文档"><a href="#Python文档" class="headerlink" title="Python文档"></a>Python文档</h1><p><a href="python.usyiyi.cn">Python一译中文文档</a></p>
<h1 id="Python小技巧"><a href="#Python小技巧" class="headerlink" title="Python小技巧"></a>Python小技巧</h1><p><a href="https://www.cnblogs.com/hanxiaoyi/p/7745003.html" target="_blank" rel="noopener">python奇技淫巧</a></p>
<p>Python 3.X 里不包含字典类型的has_key() 函数，被 <strong>contains</strong>(key) 替代</p>
<p>Django</p>
<p><a href="https://github.com/Nyloner/NyBlog" target="_blank" rel="noopener">Nyloner/NyBlog</a></p>
<h3 id="Python-FAQ"><a href="#Python-FAQ" class="headerlink" title="Python FAQ"></a>Python FAQ</h3><p><a href="http://ictar.github.io/2016/11/19/%E4%B8%80%E6%AC%A1%E6%8A%93%E8%99%AB%E5%BC%95%E5%8F%91%E7%9A%84%E5%AF%B9python%E5%AF%BC%E5%85%A5%E6%9C%BA%E5%88%B6%E7%9A%84%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86/" target="_blank" rel="noopener">一次抓虫引发的对python导入机制的初步认识</a></p>
<h3 id="编程指南"><a href="#编程指南" class="headerlink" title="编程指南"></a>编程指南</h3><p><a href="https://zhuanlan.zhihu.com/p/19959253" target="_blank" rel="noopener">编程入门指南 v1.5</a></p>
<p><a href="http://blog.csdn.net/a910626/article/details/45223657" target="_blank" rel="noopener">知乎萧井陌大神《编程入门指南v1.3》思维导图</a></p>
<p><a href="https://zhu327.github.io/2016/06/16/python%E5%8D%8F%E7%A8%8B/" target="_blank" rel="noopener">Python协程</a></p>
<p><a href="http://www.dongwm.com/archives/%E4%BD%BF%E7%94%A8Python%E8%BF%9B%E8%A1%8C%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-asyncio%E7%AF%87/" target="_blank" rel="noopener">使用Python进行并发编程-asyncio篇(一)</a></p>
<p><a href="https://github.com/aio-libs" target="_blank" rel="noopener">aio-libs</a></p>
<p><a href="http://aosabook.org/en/500L/a-web-crawler-with-asyncio-coroutines.html" target="_blank" rel="noopener">A Web Crawler With asyncio Coroutines<br>A. Jesse Jiryu Davis and Guido van Rossum</a></p>
<h3 id="性能调试"><a href="#性能调试" class="headerlink" title="性能调试"></a>性能调试</h3><p><a href="https://zhuanlan.zhihu.com/p/28078824" target="_blank" rel="noopener">Python web 应用性能调优</a></p>
<p><a href="https://mp.weixin.qq.com/s/_azeNFiDsFpEs_lWO0mH_A" target="_blank" rel="noopener">http服务跟踪及调试工具</a></p>
<p><a href="https://github.com/uber/pyflame" target="_blank" rel="noopener">Pyflame: A Ptracing Profiler For Python</a></p>
<p>FAQ</p>
<p><a href="https://zhuanlan.zhihu.com/p/32446353" target="_blank" rel="noopener">Supervisor 的问题 minfds</a></p>

      
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Mac 键盘快捷键" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/07/24/Mac 键盘快捷键/" class="article-date">
      <time datetime="2018-07-24T01:20:41.417Z" itemprop="datePublished">2018-07-24</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>date: 2017-07-23 09:55</p>
<h4 id="Mac-的启动组合键"><a href="#Mac-的启动组合键" class="headerlink" title="Mac 的启动组合键"></a><a href="https://support.apple.com/zh-cn/HT201255" target="_blank" rel="noopener">Mac 的启动组合键</a></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Command (⌘)</span><br><span class="line">Command-R	从内建的 macOS 恢复系统启动。或者，您也可以使用 Option-Command-R 或 Shift-Option-Command-R 以通过互联网从 macOS 恢复功能启动。macOS 恢复功能可以安装不同版本的 macOS，具体取决于您在电脑启动时使用的组合键。</span><br></pre></td></tr></table></figure>
<p>PC键盘： Win键 + R</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Option (⌥)	启动进入启动管理器，您可以从中选取其他启动磁盘（若可用）。</span><br></pre></td></tr></table></figure>
<p>PC键盘： Alt键</p>
<h4 id="Mac-键盘快捷键"><a href="#Mac-键盘快捷键" class="headerlink" title="Mac 键盘快捷键"></a><a href="https://support.apple.com/zh-cn/HT201236" target="_blank" rel="noopener">Mac 键盘快捷键</a></h4><h5 id="修饰键："><a href="#修饰键：" class="headerlink" title="修饰键："></a>修饰键：</h5><p>Command ⌘</p>
<p>Shift ⇧</p>
<p>Option ⌥</p>
<p>Control ⌃</p>
<p>Caps Lock ⇪</p>
<p>Fn</p>

      
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Python爬虫面试题" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/07/24/Python爬虫面试题/" class="article-date">
      <time datetime="2018-07-24T01:20:41.417Z" itemprop="datePublished">2018-07-24</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Python面试题</p>
<p><a href="http://blog.csdn.net/u014745194/article/details/78835270" target="_blank" rel="noopener">python面试题系列之一</a></p>
<p><a href="http://www.jianshu.com/p/be60981eed2b" target="_blank" rel="noopener">python爬虫(一)_爬虫原理和数据抓取</a></p>
<p><a href="http://blog.csdn.net/u014745194/article/details/75258491" target="_blank" rel="noopener">Python爬虫简述系列之一</a></p>
<p><a href="http://blog.csdn.net/qq_36143300/article/details/54928389" target="_blank" rel="noopener">数据科学工程师面试宝典系列之一–Python爬虫实战</a></p>
<p>数据分析简历<br><a href="https://erlemar.github.io/" target="_blank" rel="noopener">Data science portfolio by Andrey Lukyanenko</a></p>
<p><a href="https://github.com/rushter/data-science-blogs" target="_blank" rel="noopener">rushter/data-science-blogs</a></p>
<p><a href="http://www.jianshu.com/p/97e10182c38a" target="_blank" rel="noopener">数据科学家的成长之路[译]</a></p>

      
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Python开发者" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/07/24/Python开发者/" class="article-date">
      <time datetime="2018-07-24T01:20:41.417Z" itemprop="datePublished">2018-07-24</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>优秀Python开发者</p>
<p>Flask和Requests的作者<br>Celery作者Ask Solem<br>Django<br>kombu</p>
<p>爬虫<br>刚开始写爬虫用的是urllib2，后来知道了requests，惊为天人。刚开始解析网页用的是re，后来知道了BeautifulSoup，解析页面不能再轻松。再后来看别人的爬虫，知道了scrapy，被这个框架惊艳到了。之后遇到了一些有验证码的网站，于是知道了PIL。但后来知道了opencv，pybrain。当在爬虫中用上人工神经网络识别出验证码，兴奋得守在爬虫旁边看他爬完全站。再后来知道了threading，知道了celery。不断的学习，不断的接触和知道更多的东西，爬虫与反爬虫的对抗会一直进行下去。</p>
<p>作者：bsdr<br>链接：<a href="https://www.zhihu.com/question/38192299/answer/75884957" target="_blank" rel="noopener">https://www.zhihu.com/question/38192299/answer/75884957</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
<p>requests<br>BeautifulSoup<br>opencv<br>pybrain<br>threading<br>celery</p>
<p>scrapy<br>selenium<br>phantomjs</p>
<p>python学习手册 第四版</p>
<p>python核心编程 第二版</p>
<p>开始不用学，进阶学习<br>ORM<br>ElasticSearch</p>
<p>不用存储，爬去数据发送邮件</p>
<p>抓去顺序</p>
<p>开放的API<br>手机APP的API<br>Android的API实现抓取知乎的数据<br>HTML</p>
<p>抓包工具<br>Charles</p>
<p>lxml<br>xpath</p>
<p>前端知识</p>
<p>运维-&gt;运维开发-&gt;Web开发-&gt;数据分析-&gt;需要数据写爬虫</p>

      
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Python学习-老男孩系的学生们的博客" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/07/24/Python学习-老男孩系的学生们的博客/" class="article-date">
      <time datetime="2018-07-24T01:20:41.417Z" itemprop="datePublished">2018-07-24</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p><a href="http://www.cnblogs.com/yangjian1/p/5917621.html" target="_blank" rel="noopener">python基础之初始python</a></p>
<p><a href="http://www.cnblogs.com/allen-zhang/p/6114864.html" target="_blank" rel="noopener">Python之路,Day1 - Python基础1</a></p>

      
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Python分布式爬虫打造搜索引擎笔记" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/07/24/Python分布式爬虫打造搜索引擎笔记/" class="article-date">
      <time datetime="2018-07-24T01:20:41.417Z" itemprop="datePublished">2018-07-24</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>2018.03.19 20:01</p>
<p>作者博客<br><a href="http://projectsedu.com" target="_blank" rel="noopener">http://projectsedu.com</a><br>github.com/liyaopinner</p>
<p>IDE pycharm<br>数据库 mysql redis </p>
<p>Python 2.7.13<br>Python 3.5.3</p>
<p>第三章 基础知识</p>
<p>技术选型:<br>scrapy<br>requests<br>scrapy selector</p>
<p>网页分类：<br>静态网页<br>动态网页<br>微博service(rest api)</p>
<p>爬虫能做什么<br>1、搜索引擎-百度<br>2、推荐引擎-今日头条<br>3、机器学习的数据样本<br>4、数据分析、舆情分析</p>
<p>正则表达式<br>非贪婪模式 ？<br>至少出现一次 +<br>限定前面的字符出现的次数 {}<br>满足任意一个 []<br>\s 代表空格<br>\S 只要不是空格就行一个字符<br>\w 代表a-zA-Z0-9_ 代表字符<br>\W 不代表a-Z0-9_<br>\d 代表数字</p>
<p>深度优先和广度优先算法</p>
<p>深度优先 递归实现<br>广度优先 队列实现</p>
<p>数据结构里很重要的算法</p>
<p>深度优先过程</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">depth_tree</span><span class="params">(tree_node)</span>:</span></span><br><span class="line">	<span class="keyword">if</span> tree_node <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">		print(tree_node._data)</span><br><span class="line">		<span class="keyword">if</span> tree_node._left <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">			<span class="keyword">return</span> depth_tree(tree_node._left):</span><br><span class="line">		<span class="keyword">if</span> tree_node._right <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">			<span class="keyword">return</span> depth_tree(tree_node._right)</span><br></pre></td></tr></table></figure>
<p>广度优先过程</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">level_queue</span><span class="params">(root)</span>:</span></span><br><span class="line">	<span class="keyword">if</span> root <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	my_queue = []</span><br><span class="line">	node = root</span><br><span class="line">	my_queue.append(node)</span><br><span class="line">	<span class="keyword">while</span> my_queue:</span><br><span class="line">		node = my_queue.pop(<span class="number">0</span>)</span><br><span class="line">		print(node.elem)</span><br><span class="line">		<span class="keyword">if</span> node.lchild <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">			my_queue.append(node.lchild)</span><br><span class="line">		<span class="keyword">if</span> node.rchild <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">			my_queue.append(node.rchild)</span><br></pre></td></tr></table></figure>
<p>url去重策略</p>
<p>1、将访问过的url保存到数据库<br>2、将访问过的url保存到set中<br>3、url经过md5等方法哈希后保存到set中 scrapy使用的是这个方法<br>4、用bitmap方法，<br>5、bloomfilter方法对bitmap进行改进，多重hash函数降低冲突</p>
<p>unicode和utf8编码<br>内存中的数据都是unicode编码<br>python3中文件中存的数据是utf-8编码<br>所有编码解码 decode为unicode<br>uncode编码 encode为utf-8</p>
<p>获取系统默认的编码<br>sys.getdefaultencoding()</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-*- coding: utf<span class="number">-8</span> -*-</span><br></pre></td></tr></table></figure>
<p>scrapy爬取jobbole</p>
<p>pycharm 中调试scrapy</p>
<p>main.py main.py放在和scrapy.cfg同一目录</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.cmdline <span class="keyword">import</span> execute</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找main.py的执行位置</span></span><br><span class="line">sys.path.append(os.path.dirname(os.path.abspath(__file__)))</span><br><span class="line"><span class="comment"># 执行爬虫</span></span><br><span class="line">execute([<span class="string">'scrapy'</span>, <span class="string">'crawl'</span>, <span class="string">'jobbole'</span>])</span><br></pre></td></tr></table></figure>
<p>xpath语法<br>div[@class=”goods-add fn-clear J-DAddToBag”]<br>//span[contains(@class, ‘J-DAddToBag’)]</p>
<p>/article/div[1] 选取属于article子元素的第一个div元素<br>/article/div[last()] 选取属于article子元素的最后一个div元素<br>/article/div[last()-1] 选取属于article子元素的倒数第二个div元素<br>//div/<em> 选取属于div元素的所有子节点<br>//div[@</em>] 选取所有带属性的title元素<br>/div/a|//div/p 选取所有div元素的a和p元素<br>//span|//ul 选取文档中的span和ul元素<br>article/div/p| //span 选取所有属于span元素的div元素的p元素以及文档中的所有的span元素</p>
<p>css selector语法<br>li a 选取所有li下的所有a元素<br>ul + p 选择ul后面的第一个p元素<br>div#container &gt; ul 选取id为container的div的第一个ul子元素<br>ul ~ p 选取与ul相邻的所有p元素<br>a[title] 选取所有有title属性的a元素<br>a[href=”<a href="http://jobbole.com&quot;]" target="_blank" rel="noopener">http://jobbole.com&quot;]</a> 选取所有href属性为jobbole.com值的a元素<br>a[href*=”jobbole”] 选取所有href属性包含jobbole.com的a元素<br>a[href^=”http”] 选取所有href属性以http开头的a元素<br>a[href$=”.jpg”] 选取所有href属性以.jpg结尾的a元素<br>input[type=radio]:checked 选择选中的radio的元素<br>div:not(#container) 选取所有id非container属性的div元素<br>li:nth-child(3) 选取第三个li元素<br>tr:nth-child(2n) 第偶数个tr</p>
<p>伪类选择器<br>h1::text<br>div h1::attr(href) </p>
<p>列表生成式<br>tag_list = [‘职场’, ‘1 评论’, ‘fuck两点水’]</p>
<p>[ element for element in tag_list if not element.strip().endswith(“评论”)]</p>
<p>正则表达式<br>fav_nums = “sssd123werwe”<br>match_re = re.match(“.<em>(\d+).</em>“, fav_nums)<br>if match_re:<br>    fav_nums = int(match_re.group(1))<br>else:<br>    fav_nums = 0</p>
<p>字符串方法<br>tags = “,”.join(‘12345’)        结果 ‘1,2,3,4,5’</p>
<p>response.xpath().extract() 需要异常处理<br>response.xpath().extract_first()</p>
<p>from scrapy.http import Request<br>Request(url=,callback=)</p>
<p>from urllib import parse<br>parse.urljoin()</p>
<p>meta<br>scrapy.Request(url=,meta={‘goodsid’:goodsid},callback=)<br>goodsid = response.meta.get(“goodsid”, “”)</p>
<p>scrapy 图片管道 imagespipeline <code>pip install pillow</code><br>settings.py<br>scrapy.pipelines.images.ImagesPipeLine<br>IMAGES_URLS_FIELD = “items.py 中的item,url需要是list类型”<br>project_dir = os.path.abspath(os.path.dirname(<strong>file</strong>))<br>IMAGES_STORE = os.path.join(project_dir, ‘images’)</p>
<p><a href="https://coding.imooc.com/lesson/92.html#mid=2878" target="_blank" rel="noopener">DIY pipeline</a><br>图片管道自定义<br>4-12 items设计-3 视频的02:12</p>
<p>ImagesPipeline<br>item_completed()</p>
<p>utils/<br><strong>init</strong>.py<br>common.py</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_md5</span><span class="params">(url)</span>:</span></span><br><span class="line">	<span class="keyword">if</span> isinstance(url, str):</span><br><span class="line">		url = url.encode(<span class="string">"utf-8"</span>)</span><br><span class="line">	m = hashlib.md5()</span><br><span class="line">	m.update(url)</span><br><span class="line">	<span class="keyword">return</span> m.hexdigest()</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">	print(get_md5(<span class="string">"http://www.baidu.com"</span>)</span><br></pre></td></tr></table></figure>
<p>数据保存<br>pipeline</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">		self.file = codecs.open(<span class="string">'article.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">"utf-8"</span>)</span><br><span class="line">		</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">	lines = json.dumps(dict(item), ensure_ascii=<span class="keyword">False</span>) + <span class="string">"\n"</span></span><br><span class="line">	self.write(lines)</span><br><span class="line">	<span class="keyword">return</span> item</span><br><span class="line">	</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">            self.file.close()</span><br></pre></td></tr></table></figure>
<p>JsonItemExporter</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> JsonItemExporter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonExporterPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">		self.file = open(<span class="string">'articleexporter.json'</span>, <span class="string">'wb'</span>)</span><br><span class="line">		self.exporter = JsonItemExporter(self.file, encoding=<span class="string">"utf-8"</span>, ensure_ascii=<span class="keyword">False</span>)</span><br><span class="line">		self.exporter.start_exporting()</span><br><span class="line">	</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self. item, spider)</span>:</span></span><br><span class="line">		self.exporter.export_item(item)</span><br><span class="line">		<span class="keyword">return</span> item</span><br><span class="line">		</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">		self.exporter.finish_exporting()</span><br><span class="line">		self.file.close()</span><br></pre></td></tr></table></figure>
<p>数据库</p>
<p>新建数据库<br>新建数据表</p>
<p><img src="./jobbole_article_mysql.png" alt="jobbole_article_mysql"></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create_date = <span class="string">'时间字符串'</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">	datetime.datetime.strptime(create_date, <span class="string">"%Y/%m/%d"</span>).date()</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">	create_date = datetime.datetime.now().date()</span><br></pre></td></tr></table></figure>
<p>pip install mysqlclient pymysql</p>
<p>同步插入mysql</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">		self.conn = pymysql.connect(<span class="string">'host'</span>, <span class="string">'user'</span>, <span class="string">'password'</span>, <span class="string">'port'</span>, <span class="string">'dbname'</span>, charset=<span class="string">"utf-8"</span>, use_unicode=Ture)</span><br><span class="line">		self.cursor = self.conn.cursor()</span><br><span class="line">		</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">		insert_sql = <span class="string">"""</span></span><br><span class="line"><span class="string">			insert into jobbole_article(title, url)</span></span><br><span class="line"><span class="string">			VALUES (%s,%s)"""</span></span><br><span class="line">		self.cursor.execute(insert_sql, (item[<span class="string">"title"</span>], item[<span class="string">"url"</span>])</span><br><span class="line">		self.conn.commit()</span><br></pre></td></tr></table></figure>
<p>异步插入mysql</p>
<p>连接池</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> twisted.enterprise <span class="keyword">import</span> adbapi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlTwistedPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">		self.dbpool = dbpool</span><br><span class="line">	</span><br><span class="line"><span class="meta">	@classmethod</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span></span><br><span class="line">		dbparms = dict(</span><br><span class="line">			host = settings[<span class="string">"MYSQL_HOST"</span>],</span><br><span class="line">			db = settings[<span class="string">"MYSQL_DBNAME"</span>],</span><br><span class="line">			user = settings[<span class="string">"MYSQL_USER"</span>],</span><br><span class="line">			password = settings[<span class="string">"MYSQL_PASSWORD"</span>],</span><br><span class="line">			charset = <span class="string">'utf-8'</span>,</span><br><span class="line">			cursorclass = pymysql.cursors.DictCursor,</span><br><span class="line">			use_unicode = <span class="keyword">True</span>,</span><br><span class="line">		)</span><br><span class="line">		dbpool = adbapi.ConnectionPool(<span class="string">"pymysql"</span>, **dbparms)</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">return</span> cls(dbpool)</span><br><span class="line">	</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">		query = self.dbpool.runInteraction(do_insert, item)</span><br><span class="line">		query.addErrback(self.handle_error)</span><br><span class="line">		</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">handle_error</span><span class="params">(self, failure)</span>:</span></span><br><span class="line">		print(failure)</span><br><span class="line">		</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">do_insert</span><span class="params">(self, cursor, item)</span>:</span></span><br><span class="line">		insert_sql = <span class="string">"""</span></span><br><span class="line"><span class="string">			insert into jobbole_article(title, url)</span></span><br><span class="line"><span class="string">			VALUES (%s,%s)"""</span></span><br><span class="line">		cursor.execute(insert_sql, (item[<span class="string">"title"</span>], item[<span class="string">"url"</span>])</span><br></pre></td></tr></table></figure>
<p>scrapy-djangoitem</p>
<p>pip install scrapy-djangoitem</p>
<p>scrapy item loader 机制</p>
<p>itemloader会将所有提取的值变为list类型</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">item_loader = ItemLoader(item=JobboleItem(),response=response)</span><br><span class="line"></span><br><span class="line">itemloader.add_xpath()</span><br><span class="line">itemloader.add_css()</span><br><span class="line">itemloader.value()</span><br><span class="line"></span><br><span class="line">item = itemloader.load_item()</span><br><span class="line"></span><br><span class="line"><span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>
<p>items.py</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> MapCompose</span><br><span class="line"></span><br><span class="line"><span class="comment">#取第一个值</span></span><br><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> TakeFirst，Join</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_jobbole</span><span class="params">(value)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> value + <span class="string">"-jobbole"</span></span><br><span class="line">	</span><br><span class="line"></span><br><span class="line">title = scrapy.Field(</span><br><span class="line">	input_processor = MapCompose(add_jobbole)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">name = scrapy.Field(</span><br><span class="line">	input_processor = MapCompose(<span class="keyword">lambda</span> x:x+<span class="string">"-jinlong"</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">urls = scrapy.Field(</span><br><span class="line">	input_processor = MapCompose(<span class="keyword">lambda</span> x:x+<span class="string">"-kjj"</span>, add_jobbole)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">create_date = scrapy.Field(</span><br><span class="line">	input_processor = MapCompose(add_jobbole)</span><br><span class="line">	output_processor = TakeFirst()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>自定义itemloader</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleItemLoader</span><span class="params">(ItemLoader)</span>:</span></span><br><span class="line">	default_output_processor = TakeFirst()</span><br></pre></td></tr></table></figure>
<p>session和cookie自动登录机制<br>session服务器端<br>cookie客户端</p>
<p>settings.py<br>AUTOTHROTTLE_ENABLED = True</p>
<p>spider.py</p>
<p>custom_settings = {<br>    “COOKIES_ENABLED”: True<br>}</p>
<p>selenuim driver</p>
<p>ChromeDriver - WebDriver for Chrome</p>
<p><a href="https://sites.google.com/a/chromium.org/chromedriver/downloads" target="_blank" rel="noopener">https://sites.google.com/a/chromium.org/chromedriver/downloads</a></p>
<p>brew install chromedriver</p>
<p><a href="https://segmentfault.com/a/1190000013067705" target="_blank" rel="noopener">https://segmentfault.com/a/1190000013067705</a></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.options <span class="keyword">import</span> Options</span><br><span class="line">chrome_options = Options()</span><br><span class="line">chrome_options.add_argument(<span class="string">'window-size=1920x3000'</span>) <span class="comment">#指定浏览器分辨率</span></span><br><span class="line">chrome_options.add_argument(<span class="string">'--disable-gpu'</span>) <span class="comment">#谷歌文档提到需要加上这个属性来规避bug</span></span><br><span class="line">chrome_options.add_argument(<span class="string">'--hide-scrollbars'</span>) <span class="comment">#隐藏滚动条, 应对一些特殊页面</span></span><br><span class="line">chrome_options.add_argument(<span class="string">'blink-settings=imagesEnabled=false'</span>) <span class="comment">#不加载图片, 提升速度</span></span><br><span class="line">chrome_options.add_argument(<span class="string">'--headless'</span>) <span class="comment">#浏览器不提供可视化页面. linux下如果系统不支持可视化不加这条会启动失败</span></span><br><span class="line">chrome_options.binary_location = <span class="string">r'/Applications/Google Chrome Canary.app/Contents/MacOS/Google Chrome Canary'</span> <span class="comment">#手动指定使用的浏览器位置</span></span><br></pre></td></tr></table></figure>
      
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Nginx-1.13.11" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/07/24/Nginx-1.13.11/" class="article-date">
      <time datetime="2018-07-24T01:20:41.417Z" itemprop="datePublished">2018-07-24</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>vim /etc/yum.repos.d/nginx.repo</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[nginx]</span><br><span class="line">name=nginx repo</span><br><span class="line">baseurl=http://nginx.org/packages/mainline/centos/7/$basearch/</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>
<p><code>yum install nginx -y</code></p>
<p>查看nginx版本 <code>nginx -v</code><br>获取编译参数 <code>nginx -V</code><br>文件位置<br>/usr/lib/systemd/system/nginx.service<br>user  nginx</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=nginx - high performance web server</span><br><span class="line">Documentation=http://nginx.org/en/docs/</span><br><span class="line">After=network-online.target remote-fs.target nss-lookup.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=forking</span><br><span class="line">PIDFile=/var/run/nginx.pid</span><br><span class="line">ExecStart=/usr/sbin/nginx -c /etc/nginx/nginx.conf</span><br><span class="line">ExecReload=/bin/kill -s HUP $MAINPID</span><br><span class="line">ExecStop=/bin/kill -s TERM $MAINPID</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/<span class="built_in">log</span>/nginx/error.log --http-log-path=/var/<span class="built_in">log</span>/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt=<span class="string">'-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -fPIC'</span> --with-ld-opt=<span class="string">'-Wl,-z,relro -Wl,-z,now -pie'</span></span><br></pre></td></tr></table></figure>
<h2 id="编译安装nginx"><a href="#编译安装nginx" class="headerlink" title="编译安装nginx"></a>编译安装nginx</h2><p>安装依赖环境</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y make git pcre-devel zlib-devel</span><br></pre></td></tr></table></figure>
<p>添加nginx用户和组</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">useradd -s /sbin/nologin -M nginx</span><br></pre></td></tr></table></figure>
<p>获取源码：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> -b 1.13.12 https://github.com/nginx/nginx.git</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/openssl/openssl.git</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/grahamedgecombe/nginx-ct.git</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/google/ngx_brotli.git&amp;&amp;<span class="built_in">cd</span> ngx_brotli&amp;&amp;git submodule update --init&amp;&amp;<span class="built_in">cd</span> ..</span><br></pre></td></tr></table></figure>
<p>编译</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> nginx</span><br><span class="line"></span><br><span class="line">auto/configure --prefix=/usr/<span class="built_in">local</span>/nginx-1.13.11 --user=nginx --group=nginx --add-module=../ngx_brotli --add-module=../nginx-ct --with-openssl=../openssl --with-openssl-opt=<span class="string">'enable-tls1_3 enable-weak-ssl-ciphers'</span> --with-http_v2_module --with-http_ssl_module --with-http_gzip_static_module</span><br><span class="line"></span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure>
<p>/usr/lib/systemd/system/nginx.service</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cat &gt;/usr/lib/systemd/system/nginx.service&lt;&lt;EOF</span><br><span class="line">[Unit]</span><br><span class="line">Description=nginx - high performance web server</span><br><span class="line">Documentation=http://nginx.org/en/docs/</span><br><span class="line">After=network-online.target remote-fs.target nss-lookup.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=forking</span><br><span class="line">PIDFile=/var/run/nginx.pid</span><br><span class="line">ExecStartPre=/usr/local/nginx-1.13.11/sbin/nginx -t -c /usr/local/nginx-1.13.11/conf/nginx.conf</span><br><span class="line">ExecStart=/usr/local/nginx-1.13.11/sbin/nginx -c /usr/local/nginx-1.13.11/conf/nginx.conf</span><br><span class="line">ExecReload=/usr/local/nginx-1.13.11/sbin/nginx -s reload</span><br><span class="line">ExecStop=/usr/local/nginx-1.13.11/sbin/nginx -s quit</span><br><span class="line">PrivateTmp=true</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<p>配置</p>
<p>/usr/local/nginx-1.13.11/conf/nginx.conf</p>
<h3 id="使用acme-sh工具获取ssl证书"><a href="#使用acme-sh工具获取ssl证书" class="headerlink" title="使用acme.sh工具获取ssl证书"></a>使用<a href="https://github.com/Neilpang/acme.sh" target="_blank" rel="noopener">acme.sh</a>工具获取ssl证书</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl https://get.acme.sh | sh</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1. DNSPod API Key and ID</span></span><br><span class="line"><span class="built_in">export</span> DP_Id=<span class="string">""</span></span><br><span class="line"><span class="built_in">export</span> DP_Key=<span class="string">""</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2.</span></span><br><span class="line">acme.sh --issue --dns dns_dp --nginx --keylength 4096 -d quanjinlong.cn -d www.quanjinlong.cn -d blog.quanjinlong.cn</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.</span></span><br><span class="line">acme.sh --issue --dns dns_dp --nginx --keylength 4096 -d quanjinlong.cn -d www.quanjinlong.cn -d blog.quanjinlong.cn --keylength ec-256</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.</span></span><br><span class="line">acme.sh --installcert -d quanjinlong.cn -d www.quanjinlong.cn -d blog.quanjinlong.cn \</span><br><span class="line">        --key-file   /data/ssl/quanjinlong.cn/quanjinlong_cn_rsa.key \</span><br><span class="line">        --fullchain-file /data/ssl/quanjinlong.cn/quanjinlong_cn_rsa_fullchain.cer \</span><br><span class="line">        --reloadcmd  <span class="string">"systemctl restart nginx.service"</span></span><br><span class="line">      </span><br><span class="line"><span class="comment">#4.        </span></span><br><span class="line">acme.sh --install-cert -d quanjinlong.cn -d www.quanjinlong.cn -d blog.quanjinlong.cn \</span><br><span class="line">--key-file       /data/ssl/quanjinlong.cn/quanjinlong_cn_rsa_key.pem  \</span><br><span class="line">--fullchain-file /data/ssl/quanjinlong.cn/quanjinlong_cn_rsa_fullchain_cert.pem \</span><br><span class="line">--reloadcmd     <span class="string">"systemctl restart nginx.service"</span></span><br></pre></td></tr></table></figure>
<p>日志自动切分</p>
<p>vim /etc/logrotate.d/nginx</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">/data/nginx/log/*.log &#123;</span><br><span class="line">    su root root</span><br><span class="line">    daily</span><br><span class="line">    rotate 5</span><br><span class="line">    missingok</span><br><span class="line">    notifempty</span><br><span class="line">    sharedscripts</span><br><span class="line">    dateext</span><br><span class="line">    postrotate</span><br><span class="line">        if [ -f /var/run/nginx.pid ]; then</span><br><span class="line">            kill -USR1 `cat /var/run/nginx.pid`</span><br><span class="line">        fi</span><br><span class="line">    endscript</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>手动执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/sbin/logrotate -f /etc/logrotate.d/nginx</span><br></pre></td></tr></table></figure>
<p>启动</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">systemctl start nginx.service</span><br><span class="line">systemctl restart nginx.service</span><br><span class="line">systemctl stop nginx.service</span><br><span class="line">systemctl status nginx.service</span><br><span class="line">systemctl <span class="built_in">enable</span> nginx.service</span><br></pre></td></tr></table></figure>
<p><a href="https://www.ssllabs.com/ssltest/" target="_blank" rel="noopener">ssl 测试工具</a></p>
<ul>
<li><p>TLS 1.3<br><a href="https://imququ.com/post/enable-tls-1-3.html" target="_blank" rel="noopener">本博客开始支持 TLS 1.3</a>|<a href="https://www.coldawn.com/tag/draft-23/" target="_blank" rel="noopener">CentOS 7 编译安装nginx并启用TLS1.3</a></p>
</li>
<li><p>CT<br><a href="https://imququ.com/post/certificate-transparency.html#toc-2" target="_blank" rel="noopener">通过 nginx-ct 启用 CT</a></p>
</li>
</ul>
<p><a href="https://ct.grahamedgecombe.com/" target="_blank" rel="noopener">Certificate Transparency Monitor</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">yum -y install golang</span><br><span class="line"></span><br><span class="line">wget -O ct-submit.zip -c https://github.com/grahamedgecombe/ct-submit/archive/v1.1.2.zip</span><br><span class="line">unzip ct-submit.zip</span><br><span class="line"><span class="built_in">cd</span> ct-submit-1.1.2</span><br><span class="line">go build</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#rsa</span><br><span class="line">./ct-submit-1.1.2 ct.googleapis.com/icarus &lt;/data/ssl/quanjinlong.cn/quanjinlong_cn_rsa_fullchain_cert.pem &gt;/data/ssl/quanjinlong.cn/scts/icarus.sct</span><br><span class="line"></span><br><span class="line">./ct-submit-1.1.2 ct1.digicert-ct.com/log &lt;/data/ssl/quanjinlong.cn/quanjinlong_cn_rsa_fullchain_cert.pem &gt;/data/ssl/quanjinlong.cn/scts/digicert.sct</span><br><span class="line"></span><br><span class="line">./ct-submit-1.1.2 mammoth.ct.comodo.com &lt;/data/ssl/quanjinlong.cn/quanjinlong_cn_rsa_fullchain_cert.pem &gt;/data/ssl/quanjinlong.cn/scts/comodo.sct</span><br><span class="line"></span><br><span class="line">#ecc</span><br><span class="line">./ct-submit-1.1.2 ct.googleapis.com/icarus &lt;/data/ssl/quanjinlong.cn/quanjinlong_cn_ecc_fullchain_cert.pem &gt;/data/ssl/quanjinlong.cn/scts/icarus_ecc.sct</span><br><span class="line"></span><br><span class="line">./ct-submit-1.1.2 ct1.digicert-ct.com/log &lt;/data/ssl/quanjinlong.cn/quanjinlong_cn_ecc_fullchain_cert.pem &gt;/data/ssl/quanjinlong.cn/scts/digicert_ecc.sct</span><br><span class="line"></span><br><span class="line">./ct-submit-1.1.2 mammoth.ct.comodo.com &lt;/data/ssl/quanjinlong.cn/quanjinlong_cn_ecc_fullchain_cert.pem &gt;/data/ssl/quanjinlong.cn/scts/comodo_ecc.sct</span><br></pre></td></tr></table></figure>
<ul>
<li><a href="">HSTS</a></li>
</ul>
<p>nginx server conf</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add_header               Strict-Transport-Security &quot;max-age=31536000; includeSubDomains; preload&quot;;</span><br></pre></td></tr></table></figure>
<ul>
<li><a href="https://gist.github.com/esurdam/ef72f1c47be7c074499cb920683bd307" target="_blank" rel="noopener">HKPK</a></li>
</ul>
<p><a href="https://letsencrypt.org/certificates/" target="_blank" rel="noopener">Chain of Trust - Let’s Encrypt - Free SSL/TLS Certificates</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget -O lets-encrypt-x3-cross-signed.pem https://letsencrypt.org/certs/lets-encrypt-x3-cross-signed.pem.txt</span><br><span class="line"></span><br><span class="line">wget -O lets-encrypt-x4-cross-signed.pem https://letsencrypt.org/certs/lets-encrypt-x4-cross-signed.pem.txt</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">openssl x509 -noout -in lets-encrypt-x3-cross-signed.pem -pubkey | \</span><br><span class="line">openssl rsa -pubin -outform der | \</span><br><span class="line">openssl dgst -sha256 -binary | \</span><br><span class="line">base64</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">openssl x509 -noout -in lets-encrypt-x4-cross-signed.pem -pubkey | \</span><br><span class="line">openssl rsa -pubin -outform der | \</span><br><span class="line">openssl dgst -sha256 -binary | \</span><br><span class="line">base64</span><br></pre></td></tr></table></figure>
<p>nginx server conf</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add_header Public-Key-Pins &apos;pin-sha256=&quot;YLh1dUR9y6Kja30RrAn7JKnbQG/uEtLMkBgFF2Fuihg=&quot;; pin-sha256=&quot;sRHdihwgkaib1P1gxX8HFszlD+7/gTfNvuAybgLPNis==&quot;; max-age=2592000; includeSubDomains&apos;;</span><br></pre></td></tr></table></figure>
<ul>
<li><a href="https://weakdh.org/sysadmin.html" target="_blank" rel="noopener">ssl_dhparam</a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl dhparam -out dhparams.pem 2048</span><br></pre></td></tr></table></figure>
<p>nginx server conf</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssl_dhparam dhparams.pem</span><br></pre></td></tr></table></figure>
<p><a href="https://imququ.com/post/my-nginx-conf.html" target="_blank" rel="noopener">imquu.com-本博客 Nginx 配置之完整篇</a></p>

      
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Python分布式爬虫打造搜索引擎-bobby-慕课网-运行环境" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/07/24/Python分布式爬虫打造搜索引擎-bobby-慕课网-运行环境/" class="article-date">
      <time datetime="2018-07-24T01:20:41.417Z" itemprop="datePublished">2018-07-24</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Python分布式爬虫打造搜索引擎-bobby-慕课网</p>
<p>ArticleSpider运行环境</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pipenv --python=</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy redis pillow mysqlclient elasticsearch_dsl</span><br></pre></td></tr></table></figure>
<pre><code>import MySQLdb
</code></pre><p>ModuleNotFoundError: No module named ‘MySQLdb’</p>

      
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Python分布式爬虫打造搜索引擎 学习笔记" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/07/24/Python分布式爬虫打造搜索引擎 学习笔记/" class="article-date">
      <time datetime="2018-07-24T01:20:41.417Z" itemprop="datePublished">2018-07-24</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="Python分布式爬虫打造搜索引擎"><a href="#Python分布式爬虫打造搜索引擎" class="headerlink" title="Python分布式爬虫打造搜索引擎"></a><a href="http://coding.imooc.com/class/92.html" target="_blank" rel="noopener">Python分布式爬虫打造搜索引擎</a></h1><h2 id="第1章-课程介绍"><a href="#第1章-课程介绍" class="headerlink" title="第1章 课程介绍"></a>第1章 课程介绍</h2><h4 id="介绍课程目标、通过课程能学习到的内容、和系统开发前需要具备的知识"><a href="#介绍课程目标、通过课程能学习到的内容、和系统开发前需要具备的知识" class="headerlink" title="介绍课程目标、通过课程能学习到的内容、和系统开发前需要具备的知识"></a>介绍课程目标、通过课程能学习到的内容、和系统开发前需要具备的知识</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Scrapy </span><br><span class="line">elasticsearch</span><br><span class="line">django</span><br></pre></td></tr></table></figure>
<h2 id="第2章-windows下搭建开发环境"><a href="#第2章-windows下搭建开发环境" class="headerlink" title="第2章 windows下搭建开发环境"></a>第2章 windows下搭建开发环境</h2><h4 id="介绍项目开发需要安装的开发软件、-python虚拟virtualenv和-virtualenvwrapper的安装和使用、-最后介绍pycharm和navicat的简单使用"><a href="#介绍项目开发需要安装的开发软件、-python虚拟virtualenv和-virtualenvwrapper的安装和使用、-最后介绍pycharm和navicat的简单使用" class="headerlink" title="介绍项目开发需要安装的开发软件、 python虚拟virtualenv和 virtualenvwrapper的安装和使用、 最后介绍pycharm和navicat的简单使用"></a>介绍项目开发需要安装的开发软件、 python虚拟virtualenv和 virtualenvwrapper的安装和使用、 最后介绍pycharm和navicat的简单使用</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">PyCharm安装和配置</span><br><span class="line">MySQL和Navicat的安装使用</span><br><span class="line">CentOS安装Python2和Python3</span><br><span class="line">Python虚拟环境的安装配置virtualenv、virtualenvwrapper</span><br></pre></td></tr></table></figure>
<h2 id="第3章-爬虫基础知识回顾"><a href="#第3章-爬虫基础知识回顾" class="headerlink" title="第3章 爬虫基础知识回顾"></a>第3章 爬虫基础知识回顾</h2><h4 id="介绍爬虫开发中需要用到的基础知识包括爬虫能做什么，正则表达式，深度优先和广度优先的算法及实现、爬虫url去重的策略、彻底弄清楚unicode和utf8编码的区别和应用。"><a href="#介绍爬虫开发中需要用到的基础知识包括爬虫能做什么，正则表达式，深度优先和广度优先的算法及实现、爬虫url去重的策略、彻底弄清楚unicode和utf8编码的区别和应用。" class="headerlink" title="介绍爬虫开发中需要用到的基础知识包括爬虫能做什么，正则表达式，深度优先和广度优先的算法及实现、爬虫url去重的策略、彻底弄清楚unicode和utf8编码的区别和应用。"></a>介绍爬虫开发中需要用到的基础知识包括爬虫能做什么，正则表达式，深度优先和广度优先的算法及实现、爬虫url去重的策略、彻底弄清楚unicode和utf8编码的区别和应用。</h4><h5 id="技术选型"><a href="#技术选型" class="headerlink" title="技术选型"></a>技术选型</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scrapy vs requests+beautifulsoup</span><br><span class="line">1.requests和beautifulsoup都是库，scrapy是框架</span><br><span class="line">2.scrapy框架中可以加入requests和beautifulsoup</span><br><span class="line">3.scarpy基于twisted，性能是最大的优势</span><br><span class="line">4.scarpy方便扩展，提供了很多内置的功能</span><br><span class="line">5.scrapy内置的css和xpath selector非常方便，beautifulsoup最大的缺点就是慢</span><br></pre></td></tr></table></figure>
<h5 id="网页分类"><a href="#网页分类" class="headerlink" title="网页分类"></a>网页分类</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">常见类型的服务</span><br><span class="line">1.静态网页</span><br><span class="line">2.动态网页</span><br><span class="line">3.webservice(restapi)</span><br></pre></td></tr></table></figure>
<h5 id="爬虫能做什么"><a href="#爬虫能做什么" class="headerlink" title="爬虫能做什么"></a>爬虫能做什么</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">爬虫作用</span><br><span class="line">1.搜索引擎---百度、google、垂直领域搜索引擎</span><br><span class="line">2.推荐引擎---今日头条</span><br><span class="line">3.机器学习的数据样本</span><br><span class="line">4.数据分析（如金融数据分析）、舆情分析等</span><br></pre></td></tr></table></figure>
<h4 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">目录</span><br><span class="line">1.特殊字符</span><br><span class="line">1)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># 待匹配字符串</span></span><br><span class="line">line = <span class="string">'ssfder'</span></span><br><span class="line"><span class="comment"># 匹配规则</span></span><br><span class="line">regex_str = <span class="string">"^b.*"</span></span><br><span class="line"><span class="comment"># 匹配字符</span></span><br><span class="line"><span class="keyword">if</span> re.match(regex_str, line):</span><br><span class="line">	print(<span class="string">"yes"</span>)</span><br></pre></td></tr></table></figure>
<h4 id="字符串编码"><a href="#字符串编码" class="headerlink" title="字符串编码"></a>字符串编码</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.ASCII</span><br><span class="line">2.Unicode</span><br><span class="line">3.可变长编码 utf-8</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">文件：test.txt utf-8编码---&gt;(read)读取：转换为unicode编码---&gt;(read)内存中 Unicode编码---&gt;(save)保存：转换为utf-8编码</span><br></pre></td></tr></table></figure>
<h4 id="爬虫去重策略"><a href="#爬虫去重策略" class="headerlink" title="爬虫去重策略"></a>爬虫去重策略</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1.将访问过的url保存到数据库中</span><br><span class="line">2.将访问过的url保存到set中，只需要o(1)的代价就可以查询url 100000000*2byte*50个字符/1024/1024/1024 = 9GB </span><br><span class="line">3.url经过md5等方法哈希后保存到set中</span><br><span class="line">4.用bitmap方法，将访问过的url通过hash函数映射到某一位</span><br><span class="line">5.bloomfilter方法对bitmap进行改进，多重hash函数降低冲突</span><br></pre></td></tr></table></figure>
<h4 id="深度优先和广度优先"><a href="#深度优先和广度优先" class="headerlink" title="深度优先和广度优先"></a>深度优先和广度优先</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">目录</span><br><span class="line">1.网站的树结构</span><br><span class="line">2.深度优先算法和实现</span><br><span class="line">递归实现</span><br><span class="line">3.广度优先算法和实现</span><br><span class="line">队列实现</span><br></pre></td></tr></table></figure>
<h2 id="第4章-scrapy爬取知名技术文章网站"><a href="#第4章-scrapy爬取知名技术文章网站" class="headerlink" title="第4章 scrapy爬取知名技术文章网站"></a>第4章 scrapy爬取知名技术文章网站</h2><h4 id="搭建scrapy的开发环境，本章介绍scrapy的常用命令以及工程目录结构分析，本章中也会详细的讲解xpath和css选择器的使用。然后通过scrapy提供的spider完成所有文章的爬取。然后详细讲解item以及item-loader方式完成具体字段的提取后使用scrapy提供的pipeline分别将数据保存到json文件以及mysql数据库中。…"><a href="#搭建scrapy的开发环境，本章介绍scrapy的常用命令以及工程目录结构分析，本章中也会详细的讲解xpath和css选择器的使用。然后通过scrapy提供的spider完成所有文章的爬取。然后详细讲解item以及item-loader方式完成具体字段的提取后使用scrapy提供的pipeline分别将数据保存到json文件以及mysql数据库中。…" class="headerlink" title="搭建scrapy的开发环境，本章介绍scrapy的常用命令以及工程目录结构分析，本章中也会详细的讲解xpath和css选择器的使用。然后通过scrapy提供的spider完成所有文章的爬取。然后详细讲解item以及item loader方式完成具体字段的提取后使用scrapy提供的pipeline分别将数据保存到json文件以及mysql数据库中。…"></a>搭建scrapy的开发环境，本章介绍scrapy的常用命令以及工程目录结构分析，本章中也会详细的讲解xpath和css选择器的使用。然后通过scrapy提供的spider完成所有文章的爬取。然后详细讲解item以及item loader方式完成具体字段的提取后使用scrapy提供的pipeline分别将数据保存到json文件以及mysql数据库中。…</h4><h4 id="xpath"><a href="#xpath" class="headerlink" title="xpath"></a>xpath</h4><h5 id="xpath简介"><a href="#xpath简介" class="headerlink" title="xpath简介"></a>xpath简介</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.xpath使用路径表达式在xml和html中进行导航</span><br><span class="line">2.xpath包含标准函数库</span><br><span class="line">3.xpath是一个w3c的标准</span><br></pre></td></tr></table></figure>
<h5 id="xpath节点关系"><a href="#xpath节点关系" class="headerlink" title="xpath节点关系"></a>xpath节点关系</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1.父节点</span><br><span class="line">2.子节点</span><br><span class="line">3.同胞节点</span><br><span class="line">4.先辈节点</span><br><span class="line">5.</span><br></pre></td></tr></table></figure>
<h5 id="xpath语法"><a href="#xpath语法" class="headerlink" title="xpath语法"></a>xpath语法</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">表达式 说明</span><br><span class="line">article 选取所有article元素的所有子节点</span><br><span class="line">/article 选取根元素article</span><br><span class="line">article/a 选取所有属于article的子元素的a元素</span><br><span class="line">//div 选取所有div子元素（不论出现在文档任何地方）</span><br><span class="line">article//div 选取所有属于article元素的后代的div元素，不管它出现在article之下的任何位置</span><br><span class="line">//@class 选取所有名为class的属性</span><br><span class="line"></span><br><span class="line">/div/* 选取属于div元素的所有子节点</span><br><span class="line">//* 选取所有元素</span><br><span class="line">//div[@*] 选取所有带属性的title元素</span><br><span class="line">//div/a|//div/p 选取所有div元素的a和p元素</span><br><span class="line">//span|//ul 选取文档中的span和ul元素</span><br><span class="line">article/div/p|//span 选取所有属于article元素的div元素的p元素 以及文档中所有的span元素</span><br></pre></td></tr></table></figure>
<h5 id="xpath语法-谓语"><a href="#xpath语法-谓语" class="headerlink" title="xpath语法-谓语"></a>xpath语法-谓语</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">表达式 说明</span><br><span class="line">/article/div[1] 选取属于article子元素的第一个div元素</span><br><span class="line">/article/div[last()] 选取属于article子元素的最后一个div元素</span><br><span class="line">/article/div[last()-1] 选取属于article子元素的倒数第二个div元素</span><br><span class="line">//div[@lang] 选取所有拥有lang属性的div元素</span><br><span class="line">//div[@lang=&apos;eng&apos;] 选取所有lang属性为eng的div元素</span><br></pre></td></tr></table></figure>
<h5 id="scrapy-shell"><a href="#scrapy-shell" class="headerlink" title="scrapy shell"></a>scrapy shell</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell http://blog.jobbole.com/110287</span><br><span class="line"></span><br><span class="line">title = response.xpath(&quot;//div[@class=&apos;entry-header&apos;]/h1/text()&quot;)</span><br><span class="line"></span><br><span class="line">title</span><br><span class="line"></span><br><span class="line">title.extract()</span><br></pre></td></tr></table></figure>
<h4 id="css选择器"><a href="#css选择器" class="headerlink" title="css选择器"></a>css选择器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">表达式 说明</span><br><span class="line">* 选择所有节点</span><br><span class="line">#container 选择id为container的节点</span><br><span class="line">.container 选择所有class包含container的节点</span><br><span class="line">li a 选择所有li下的所有a节点</span><br><span class="line">ul + p 选择ul后面的第一个p元素</span><br><span class="line">div#container &gt; ul 选取id为container的div的第一个ul子元素</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">表达式 说明</span><br><span class="line">ul ～ p 选取与ul相邻的所有p元素</span><br><span class="line">a[title] 选取所有有title属性的a元素</span><br><span class="line">a[href=&quot;http://jobbole.com&quot;] 选取所有href属性为jobbole.com值的a元素</span><br><span class="line">a[href*=&quot;jobbole&quot;] 选取所有href属性包含jobbole的a元素</span><br><span class="line">a[href^=&quot;http&quot;] 选取所有href属性值以http开头的a元素</span><br><span class="line">a[href$=&quot;.jpg&quot;] 选取所有href属性值以.jpg结尾的a元素</span><br><span class="line">input[type=radio]:checked 选择选中的radio的元素</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">表达式 说明</span><br><span class="line">div:not(#container) 选取所有id非container的div属性</span><br><span class="line">li:nth-child(3) 选取第三个li元素</span><br><span class="line">tr:nth-child(2n) 第偶数个tr</span><br></pre></td></tr></table></figure>
<h5 id="scrapy-shell-1"><a href="#scrapy-shell-1" class="headerlink" title="scrapy shell"></a>scrapy shell</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell http://blog.jobbole.com/110287</span><br><span class="line"></span><br><span class="line">title = response.css(&quot;.entry-header h1::text&quot;)</span><br><span class="line"></span><br><span class="line">title</span><br><span class="line"></span><br><span class="line">title.extract()</span><br></pre></td></tr></table></figure>
<h4 id="scrapy-http"><a href="#scrapy-http" class="headerlink" title="scrapy.http"></a>scrapy.http</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.http import Request</span><br></pre></td></tr></table></figure>
<h4 id="scrapy-djangoitem"><a href="#scrapy-djangoitem" class="headerlink" title="scrapy-djangoitem"></a>scrapy-djangoitem</h4><h4 id="item-loader-jobbole-py"><a href="#item-loader-jobbole-py" class="headerlink" title="item-loader jobbole.py"></a>item-loader jobbole.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line"><span class="comment"># 通过item_loader加载item,JobboleArticleItem()是item.py里的item。</span></span><br><span class="line">item_loader = ItemLoader(item=JobboleArticleItem(), response=response)</span><br><span class="line">item_loader.add_css(<span class="string">"title"</span>, <span class="string">".entry-header h1::text"</span>)</span><br><span class="line">item_loader.add_xpath()</span><br><span class="line">item_loader.add_value(<span class="string">"url"</span>, response.url)</span><br><span class="line"><span class="comment"># 重新加载</span></span><br><span class="line">article_item = item_loader.load_item()</span><br><span class="line"><span class="comment"># 传出</span></span><br><span class="line"><span class="keyword">yield</span> article_item</span><br></pre></td></tr></table></figure>
<h4 id="item-py"><a href="#item-py" class="headerlink" title="item.py"></a>item.py</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">class JobBoleArticleItem(scrapy.Item):</span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    create_date = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(date_convert),</span><br><span class="line">    )</span><br><span class="line">    content = scrapy.Field()</span><br></pre></td></tr></table></figure>
<h2 id="第5章-scrapy爬取知名问答网站"><a href="#第5章-scrapy爬取知名问答网站" class="headerlink" title="第5章 scrapy爬取知名问答网站"></a>第5章 scrapy爬取知名问答网站</h2><hr>
<p>本章主要完成网站的问题和回答的提取。本章除了分析出问答网站的网络请求以外还会分别通过requests和scrapy的FormRequest两种方式完成网站的模拟登录， 本章详细的分析了网站的网络请求并分别分析出了网站问题回答的api请求接口并将数据提取出来后保存到mysql中。</p>
<hr>
<h4 id="cookie"><a href="#cookie" class="headerlink" title="cookie"></a>cookie</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">distributed_crawler_search_engine</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell -s &quot;输入headers&quot;</span><br></pre></td></tr></table></figure>
<p>知乎数据库设计</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">名 类型</span><br><span class="line">zhihu_id bigint 20 0 yes primay key 1</span><br><span class="line">url varchar 300 0 yes</span><br><span class="line">question_id bigint 20 0 yes</span><br><span class="line">author_id varchar 100 0 no</span><br><span class="line">content longtext 0 0 yes</span><br><span class="line">praise_num int 11 0 yes</span><br><span class="line">comments_num int 11 0 yes</span><br><span class="line">create_time date 0 0 yes</span><br><span class="line">update_time date 0 0 yes</span><br><span class="line">crawl_time datetime 0 0 yes</span><br><span class="line">crawl_update_time datetime 0 0 no</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> zhihu_question(<span class="string">`zhihu_id`</span> </span><br><span class="line"> )</span><br></pre></td></tr></table></figure>
<h2 id="第6章-通过CrawlSpider对招聘网站进行整站爬取"><a href="#第6章-通过CrawlSpider对招聘网站进行整站爬取" class="headerlink" title="第6章 通过CrawlSpider对招聘网站进行整站爬取"></a>第6章 通过CrawlSpider对招聘网站进行整站爬取</h2><h4 id="本章完成招聘网站职位的数据表结构设计，并通过link-extractor和rule的形式并配置CrawlSpider完成招聘网站所有职位的爬取，本章也会从源码的角度来分析CrawlSpider让大家对CrawlSpider有深入的理解。"><a href="#本章完成招聘网站职位的数据表结构设计，并通过link-extractor和rule的形式并配置CrawlSpider完成招聘网站所有职位的爬取，本章也会从源码的角度来分析CrawlSpider让大家对CrawlSpider有深入的理解。" class="headerlink" title="本章完成招聘网站职位的数据表结构设计，并通过link extractor和rule的形式并配置CrawlSpider完成招聘网站所有职位的爬取，本章也会从源码的角度来分析CrawlSpider让大家对CrawlSpider有深入的理解。"></a>本章完成招聘网站职位的数据表结构设计，并通过link extractor和rule的形式并配置CrawlSpider完成招聘网站所有职位的爬取，本章也会从源码的角度来分析CrawlSpider让大家对CrawlSpider有深入的理解。</h4><h2 id="第7章-Scrapy突破反爬虫的限制"><a href="#第7章-Scrapy突破反爬虫的限制" class="headerlink" title="第7章 Scrapy突破反爬虫的限制"></a>第7章 Scrapy突破反爬虫的限制</h2><blockquote>
<p>本章会从爬虫和反爬虫的斗争过程开始讲解，然后讲解scrapy的原理，然后通过随机切换user-agent和设置scrapy的ip代理的方式完成突破反爬虫的各种限制。本章也会详细介绍httpresponse和httprequest来详细的分析scrapy的功能，最后会通过云打码平台来完成在线验证码识别以及禁用cookie和访问频率来降低爬虫被屏蔽的可能性。…</p>
</blockquote>
<h3 id="UA-用户代理"><a href="#UA-用户代理" class="headerlink" title="UA 用户代理"></a>UA 用户代理</h3><p>hellysmile/fake-useragent</p>
<h3 id="IP代理"><a href="#IP代理" class="headerlink" title="IP代理"></a>IP代理</h3><p>西刺代理 高匿ip代理<br>aivarsk/scrapy-proxies<br>scrapy-plugins/scrapy-crawlera #收费的<br>Tor</p>
<h3 id="验证码识别"><a href="#验证码识别" class="headerlink" title="验证码识别"></a>验证码识别</h3><p>tesseract-ocr<br>在线打码<br>人工打码<br><a href="http://www.yundama.com" target="_blank" rel="noopener">云打码</a></p>
<p>settings.py<br>禁用cookie<br>自动限速</p>
<p>爬虫内写入 ，自定义设置单个爬虫<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">custom_settings = &#123;</span><br><span class="line">	<span class="string">"COOKIES_ENABLED"</span>: <span class="keyword">True</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="第8章-scrapy进阶开发"><a href="#第8章-scrapy进阶开发" class="headerlink" title="第8章 scrapy进阶开发"></a>第8章 scrapy进阶开发</h2><blockquote>
<p>本章将讲解scrapy的更多高级特性，这些高级特性包括通过selenium和phantomjs实现动态网站数据的爬取以及将这二者集成到scrapy中、scrapy信号、自定义中间件、暂停和启动scrapy爬虫、scrapy的核心api、scrapy的telnet、scrapy的web service和scrapy的log配置和email发送等。 这些特性使得我们不仅只是可以通过scrapy来完成…</p>
</blockquote>
<p>Selenium浏览器自动化测试框架<br>浏览器Drivers</p>
<h2 id="第9章-scrapy-redis分布式爬虫"><a href="#第9章-scrapy-redis分布式爬虫" class="headerlink" title="第9章 scrapy-redis分布式爬虫"></a>第9章 scrapy-redis分布式爬虫</h2><h4 id="Scrapy-redis分布式爬虫的使用以及scrapy-redis的分布式爬虫的源码分析，-让大家可以根据自己的需求来修改源码以满足自己的需求。最后也会讲解如何将bloomfilter集成到scrapy-redis中。"><a href="#Scrapy-redis分布式爬虫的使用以及scrapy-redis的分布式爬虫的源码分析，-让大家可以根据自己的需求来修改源码以满足自己的需求。最后也会讲解如何将bloomfilter集成到scrapy-redis中。" class="headerlink" title="Scrapy-redis分布式爬虫的使用以及scrapy-redis的分布式爬虫的源码分析， 让大家可以根据自己的需求来修改源码以满足自己的需求。最后也会讲解如何将bloomfilter集成到scrapy-redis中。"></a>Scrapy-redis分布式爬虫的使用以及scrapy-redis的分布式爬虫的源码分析， 让大家可以根据自己的需求来修改源码以满足自己的需求。最后也会讲解如何将bloomfilter集成到scrapy-redis中。</h4><h2 id="第10章-elasticsearch搜索引擎的使用"><a href="#第10章-elasticsearch搜索引擎的使用" class="headerlink" title="第10章 elasticsearch搜索引擎的使用"></a>第10章 elasticsearch搜索引擎的使用</h2><h4 id="本章将讲解elasticsearch的安装和使用，将讲解elasticsearch的基本概念的介绍以及api的使用。本章也会讲解搜索引擎的原理并讲解elasticsearch-dsl的使用，最后讲解如何通过scrapy的pipeline将数据保存到elasticsearch中。"><a href="#本章将讲解elasticsearch的安装和使用，将讲解elasticsearch的基本概念的介绍以及api的使用。本章也会讲解搜索引擎的原理并讲解elasticsearch-dsl的使用，最后讲解如何通过scrapy的pipeline将数据保存到elasticsearch中。" class="headerlink" title="本章将讲解elasticsearch的安装和使用，将讲解elasticsearch的基本概念的介绍以及api的使用。本章也会讲解搜索引擎的原理并讲解elasticsearch-dsl的使用，最后讲解如何通过scrapy的pipeline将数据保存到elasticsearch中。"></a>本章将讲解elasticsearch的安装和使用，将讲解elasticsearch的基本概念的介绍以及api的使用。本章也会讲解搜索引擎的原理并讲解elasticsearch-dsl的使用，最后讲解如何通过scrapy的pipeline将数据保存到elasticsearch中。</h4><h2 id="第11章-课程总结"><a href="#第11章-课程总结" class="headerlink" title="第11章 课程总结"></a>第11章 课程总结</h2><h4 id="本章讲解如何通过django快速搭建搜索网站，-本章也会讲解如何完成django与elasticsearch的搜索查询交互。"><a href="#本章讲解如何通过django快速搭建搜索网站，-本章也会讲解如何完成django与elasticsearch的搜索查询交互。" class="headerlink" title="本章讲解如何通过django快速搭建搜索网站， 本章也会讲解如何完成django与elasticsearch的搜索查询交互。"></a>本章讲解如何通过django快速搭建搜索网站， 本章也会讲解如何完成django与elasticsearch的搜索查询交互。</h4><h2 id="第12章-scrapyd部署scrapy爬虫"><a href="#第12章-scrapyd部署scrapy爬虫" class="headerlink" title="第12章 scrapyd部署scrapy爬虫"></a>第12章 scrapyd部署scrapy爬虫</h2><h4 id="本章主要通过scrapyd完成对scrapy爬虫的线上部署。"><a href="#本章主要通过scrapyd完成对scrapy爬虫的线上部署。" class="headerlink" title="本章主要通过scrapyd完成对scrapy爬虫的线上部署。"></a>本章主要通过scrapyd完成对scrapy爬虫的线上部署。</h4><h2 id="第13章-课程总结"><a href="#第13章-课程总结" class="headerlink" title="第13章 课程总结"></a>第13章 课程总结</h2><h4 id="重新梳理一遍系统开发的整个过程，-让同学对系统和开发过程有一个更加直观的理解"><a href="#重新梳理一遍系统开发的整个过程，-让同学对系统和开发过程有一个更加直观的理解" class="headerlink" title="重新梳理一遍系统开发的整个过程， 让同学对系统和开发过程有一个更加直观的理解"></a>重新梳理一遍系统开发的整个过程， 让同学对系统和开发过程有一个更加直观的理解</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install selenium redis elasticsearch_dsl PyMySQL requests</span><br></pre></td></tr></table></figure>

      
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/10/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><a class="page-number" href="/page/10/">10</a><span class="page-number current">11</span><a class="page-number" href="/page/12/">12</a><a class="page-number" href="/page/13/">13</a><span class="space">&hellip;</span><a class="page-number" href="/page/37/">37</a><a class="extend next" rel="next" href="/page/12/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                &copy; 2018 幻舞梦境
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/luuman/hexo-theme-spfk" target="_blank">spfk</a> by luuman
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" >海贼到访数: 
                            <span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>, </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit">本页阅读量: 
                            <span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>

    </div>
    <script src="https://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
<script src="/js/main.js"></script>

    <script>
        $(document).ready(function() {
            var backgroundnum = 24;
            var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
            $("#mobile-nav").css({"background-image": backgroundimg,"background-size": "cover","background-position": "center"});
            $(".left-col").css({"background-image": backgroundimg,"background-size": "cover","background-position": "center"});
        })
    </script>


<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-39498261-3', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->



	<script>
	var _hmt = _hmt || [];
	(function() {
	  var hm = document.createElement("script");
	  hm.src = "//hm.baidu.com/hm.js?1bb44bf25fea8f000a1397f9b5438de7";
	  var s = document.getElementsByTagName("script")[0]; 
	  s.parentNode.insertBefore(hm, s);
	})();
	</script>


<div class="scroll" id="scroll">
    <a href="#"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments"><i class="fa fa-comments-o"></i></a>
    <a href="#footer"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    $(document).ready(function() {
        if ($("#comments").length < 1) {
            $("#scroll > a:nth-child(2)").hide();
        };
    })
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

  <script language="javascript">
    $(function() {
        $("a[title]").each(function() {
            var a = $(this);
            var title = a.attr('title');
            if (title == undefined || title == "") return;
            a.data('title', title).removeAttr('title').hover(

            function() {
                var offset = a.offset();
                $("<div id=\"anchortitlecontainer\"></div>").appendTo($("body")).html(title).css({
                    top: offset.top - a.outerHeight() - 15,
                    left: offset.left + a.outerWidth()/2 + 1
                }).fadeIn(function() {
                    var pop = $(this);
                    setTimeout(function() {
                        pop.remove();
                    }, pop.text().length * 800);
                });
            }, function() {
                $("#anchortitlecontainer").remove();
            });
        });
    });
</script>


  </div>
</body>
</html>